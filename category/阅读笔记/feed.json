{
    "version": "https://jsonfeed.org/version/1",
    "title": "张前的小屋 • All posts by \"阅读笔记\" category",
    "description": "",
    "home_page_url": "https://ckti.github.io",
    "items": [
        {
            "id": "https://ckti.github.io/2024/10/31/MIA_2/",
            "url": "https://ckti.github.io/2024/10/31/MIA_2/",
            "title": "Membership Inference Attacks Against Recommender Systems",
            "date_published": "2024-10-31T05:54:17.000Z",
            "content_html": "<h1 id=\"background-and-problem\"><a class=\"markdownIt-Anchor\" href=\"#background-and-problem\">#</a> Background and Problem</h1>\n<h2 id=\"backgrounds-of-recommend-system\"><a class=\"markdownIt-Anchor\" href=\"#backgrounds-of-recommend-system\">#</a> Backgrounds of recommend system</h2>\n<p>The recommender system is essentially an information filtering system, relying on machine learning algorithms to predict user preferences for items. There are two main methods in these realm:</p>\n<p><strong>collaborative filtering</strong>: using traditional methods, combining the history action and similar actions to make decisions</p>\n<p><strong>content-based recommendation</strong>:  using meta-data to distinguish the user preferences.</p>\n<p>The success of recommender systems lies in the large- scale user data. However, the data in many cases contains sensitive information of individuals.</p>\n<h2 id=\"contribution-of-this-paper\"><a class=\"markdownIt-Anchor\" href=\"#contribution-of-this-paper\">#</a> Contribution of this paper</h2>\n<p>First quantification of privacy risks in recommender systems through user-level membership inference attacks.</p>\n<p>Overcoming technical challenges by addressing limited access to ranked recommendations without posterior probabilities.</p>\n<p>Introducing a shadow recommender system to generate labeled data for attack models.</p>\n<p>Extensive experiments demonstrating strong attack performance across benchmark datasets.</p>\n<p>Proposing a defense mechanism called “Popularity Randomization” to reduce attack success by adding randomness.</p>\n<h1 id=\"methods\"><a class=\"markdownIt-Anchor\" href=\"#methods\">#</a> Methods</h1>\n<h2 id=\"labeled-data-generation\"><a class=\"markdownIt-Anchor\" href=\"#labeled-data-generation\">#</a> Labeled Data Generation</h2>\n<p>The adversary builds a shadow recommender system to mimic the target recommender system and generate training data. This involves factorizing a user-item rating matrix to project users and items into a shared latent space. The adversary then calculates the center vectors of user interactions and recommendations, and the difference between these vectors forms the user feature vector. Users are labeled as members (1) or non-members (0).</p>\n<h2 id=\"attack-model-establishment\"><a class=\"markdownIt-Anchor\" href=\"#attack-model-establishment\">#</a> Attack Model Establishment</h2>\n<p>A multi-layer perceptron (MLP) with two hidden layers is used as the attack model to infer membership status. The MLP is trained on the feature vectors generated in the previous step, and outputs probabilities indicating membership.</p>\n<h2 id=\"parameter-optimization\"><a class=\"markdownIt-Anchor\" href=\"#parameter-optimization\">#</a> Parameter Optimization</h2>\n<p>The MLP is trained using stochastic gradient descent, minimizing a cross-entropy loss function. After training, the attack model uses the test data to infer the membership status of users in the target recommender system.</p>\n<h2 id=\"framework\"><a class=\"markdownIt-Anchor\" href=\"#framework\">#</a> Framework</h2>\n<p>This method leverages both user interactions and recommendation patterns, and captures the order information from the ranked lists of recommended items, a critical aspect that distinguishes it from previous membership inference attacks .</p>\n<p><img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/frameworkk_MIA4rec.png\" alt=\"The framework of the proposed method\"></p>\n<h1 id=\"experiments\"><a class=\"markdownIt-Anchor\" href=\"#experiments\">#</a> Experiments</h1>\n<h2 id=\"experimental-target\"><a class=\"markdownIt-Anchor\" href=\"#experimental-target\">#</a> Experimental target</h2>\n<p><strong>Target Models</strong>: The experiment use three Personalized recommendation algorithms  for members: Latent Factor Model (LFM) and Neural Collaborative Filtering (NCF) ; And use popularity recommendation algorithm for non-members due to the lack of non-members’ data.</p>\n<p><strong>Data Sets</strong>: This paper use three real-world datasets that is widely used for experiments of recommend systems, including Amazon Digital Music (ADM) , Lastfm-2k (lf-2k) , and Movielens-1m (ml-1m) .</p>\n<h2 id=\"data-preprocessing\"><a class=\"markdownIt-Anchor\" href=\"#data-preprocessing\">#</a> Data Preprocessing</h2>\n<p>For each dataset, the paper divide it into three disjoint subsets: shadow dataset, a target dataset and a dataset for extracting item features. Then , the paper processes these to make the data suitable:</p>\n<ol>\n<li>To generate feature vectors for users, the dataset for item feature should contain all items of the target and shadow recommenders.</li>\n<li>For the shadow or target dataset, the paper further divide it into two disjoint parts, which are used to conduct recommendations to members and non-members, respectively.</li>\n<li>The paper filtered out the users who have less than 20 interactions.</li>\n</ol>\n<h2 id=\"evaluation-metrics\"><a class=\"markdownIt-Anchor\" href=\"#evaluation-metrics\">#</a> Evaluation Metrics</h2>\n<p>This paper use AUC (area under the ROC curve) as the metric to evaluate attack performances. Regarding members as positive data points and non-members as negative data points. AUC indicates the proportion of the prediction results being positive to negative.</p>\n<h2 id=\"implementation\"><a class=\"markdownIt-Anchor\" href=\"#implementation\">#</a> Implementation</h2>\n<p>The attack model is a multi-layer perceptron (MLP) with two hidden layers. The first layer has 32 units, and the second has 8 units. Stochastic Gradient Descent (SGD) is used as the optimizer, with a learning rate of 0.01 and a momentum of 0.7. The model is trained for 20 epochs.</p>\n<h2 id=\"data-preprocessing-2\"><a class=\"markdownIt-Anchor\" href=\"#data-preprocessing-2\">#</a> Data Preprocessing</h2>\n<p>For each dataset, the paper divide it into three disjoint subsets: shadow dataset, a target dataset and a dataset for extracting item features. Then , the paper processes to make the data suitable:<br>\nTo generate feature vectors for users, the dataset for item feature should contain all items of the target and shadow recommenders.<br>\nFor the shadow or target dataset, the paper further divide it into two disjoint parts, which are used to conduct recommendations to members and non-members, respectively.<br>\nThe paper filtered out the users who have less than 20 interactions.</p>\n<h2 id=\"recommendation-performance\"><a class=\"markdownIt-Anchor\" href=\"#recommendation-performance\">#</a> Recommendation Performance</h2>\n<p>This paper use HR@100 (HR means hit rate)to evaluate the recommendation performance. The result shows that the Recommendation systems works better on Ml-1m data sets. When using Item methods , the HR is up to about 0.95</p>\n<p><img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/MIA2_RecPerf.png\" alt=\"\"></p>\n<h2 id=\"attack-performance\"><a class=\"markdownIt-Anchor\" href=\"#attack-performance\">#</a> Attack Performance</h2>\n<p>In this part, the paper evaluated three assumptions and evaluate the performance of the attack model for each assumption:</p>\n<h3 id=\"assumption-i\"><a class=\"markdownIt-Anchor\" href=\"#assumption-i\">#</a> Assumption I</h3>\n<p>the attacker knows both the algorithm and the data distribution of the target recommender system. Under this assumption, the experimental results show very strong attack performance. As the figure in the paper,when the shadow recommender system mirrors the target recommender system’s algorithm and data, the AUC scores are highly accurate.</p>\n<p><img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/MIA2_Assumption1.png\" alt=\"The result for the attack performance under Assumption I\"></p>\n<h3 id=\"assumption-ii\"><a class=\"markdownIt-Anchor\" href=\"#assumption-ii\">#</a> Assumption II</h3>\n<p>The attacker only knows the data distribution used to train the target recommender system but does not know the specific recommendation algorithm. Under this assumption, the attack performance decreases but remains strong. For example, in the ADM dataset, when the target system uses the Item algorithm and the shadow system uses the LFM algorithm, the AUC drops from 0.926 to 0.843, showing the impact of data distribution similarity on the attack.</p>\n<p><img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/MIA2_Assumption2.png\" alt=\"The result for the attack performance under Assumption II\"></p>\n<h3 id=\"assumption-iii\"><a class=\"markdownIt-Anchor\" href=\"#assumption-iii\">#</a> Assumption III</h3>\n<p>The attacker only knows the data distribution used to train the target recommender system but does not know the specific recommendation algorithm. Under this assumption, the attack performance decreases but remains strong. For example, in the ADM dataset, when the target system uses the Item algorithm and the shadow system uses the LFM algorithm, the AUC drops from 0.926 to 0.843, showing the impact of data distribution similarity on the attack.</p>\n<p><img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/MIA2_Assumption3.png\" alt=\"The result for the attack performance under Assumption III\"></p>\n<h3 id=\"hyperparameters\"><a class=\"markdownIt-Anchor\" href=\"#hyperparameters\">#</a> HyperParameters</h3>\n<p>In this paper, the impact of hyperparameters on the success of the attack is reflected in the following aspects:</p>\n<ol>\n<li><strong>Number of recommendations (k)</strong>: With more recommended items, the attack model gains more information, but performance improvements cease when the number is large enough.</li>\n<li><strong>Length of feature vectors (l)</strong>: Longer feature vectors provide more dimensional information to the model, but after a certain point, further increasing the length no longer significantly boosts performance.</li>\n<li><strong>Weights of recommended items</strong>: When considering the order of items in the recommendation list, attack performance significantly improves. Items at the front of the list are more likely to be accepted by users, and assigning higher weights to these items enhances the effectiveness of the attack.</li>\n</ol>\n<h1 id=\"defence\"><a class=\"markdownIt-Anchor\" href=\"#defence\">#</a> Defence</h1>\n<h2 id=\"popularity-randomization\"><a class=\"markdownIt-Anchor\" href=\"#popularity-randomization\">#</a> Popularity Randomization</h2>\n<p>non-members are provided with the most popular items. As a result, feature vectors of non-members are extremely similar and easily distinguished from members.  To fix this problems, the paper bring forward the popularity randomization. By selecting candidates from the most popular items randomly to make the target vectors harder to be distinguished.</p>\n<h2 id=\"evaluation-results\"><a class=\"markdownIt-Anchor\" href=\"#evaluation-results\">#</a> Evaluation results</h2>\n<p>The figure of the test result shows that when using the popularity randomization, the AUC rate was apparently downed for most of the data sets, and some of them even close to 0.5, which is nearly close to random guessing.</p>\n<p><img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/MIA2_Defence.png\" alt=\"The result of popularity randomization\"></p>\n<h1 id=\"discussion-and-conclusion\"><a class=\"markdownIt-Anchor\" href=\"#discussion-and-conclusion\">#</a> Discussion and conclusion</h1>\n<h2 id=\"the-factors-for-effects-of-attacks\"><a class=\"markdownIt-Anchor\" href=\"#the-factors-for-effects-of-attacks\">#</a> The Factors for effects of attacks</h2>\n<ol>\n<li><strong>The Choice of Datasets</strong>: The dataset with a denser user-item matrix leads to better attack performances. Attack models works better in the datasets with rich informations.</li>\n<li><strong>The Selection of Recommendation Algorithms</strong>: The recommender system with simple model structure is easier to be attacked. Compare with more complex algorithm,LFM has higher model complexity, which makes it harder to attack.</li>\n<li><strong>Distributions of Generated User Features</strong>:  When the distribution of user feature vectors generated by the shadow recommender system closely matches the target system, attack performance improves. The similarity between target data and target data is crucial for boosting attack effectiveness.</li>\n</ol>\n<h2 id=\"conclusions\"><a class=\"markdownIt-Anchor\" href=\"#conclusions\">#</a> Conclusions</h2>\n<h2 id=\"effectiveness-of-the-attack\"><a class=\"markdownIt-Anchor\" href=\"#effectiveness-of-the-attack\">#</a> Effectiveness of the Attack:</h2>\n<p>The proposed membership inference attack model demonstrates strong performance across various recommender systems and datasets. Even with limited knowledge, the model can effectively infer user membership.</p>\n<h2 id=\"proposed-defense-mechanism\"><a class=\"markdownIt-Anchor\" href=\"#proposed-defense-mechanism\">#</a> Proposed Defense Mechanism</h2>\n<p>To mitigate the attack, the authors introduce the “Popularity Randomization” defense mechanism. Experiments show that this defense significantly reduces the success rate of the attack, especially in complex models like Neural Collaborative Filtering (NCF).</p>\n<h2 id=\"future-work\"><a class=\"markdownIt-Anchor\" href=\"#future-work\">#</a> Future Work</h2>\n<p>The paper suggests further exploration of more effective defense mechanisms against such attacks and expanding the application of similar attacks and defenses to other machine learning models.</p>\n",
            "tags": []
        },
        {
            "id": "https://ckti.github.io/2024/09/28/MIA_1/",
            "url": "https://ckti.github.io/2024/09/28/MIA_1/",
            "title": "Membership Inference Attacks Against Machine Learning Models",
            "date_published": "2024-09-28T13:52:49.000Z",
            "content_html": "<h1 id=\"introduction-and-background\"><a class=\"markdownIt-Anchor\" href=\"#introduction-and-background\">#</a> Introduction and Background</h1>\n<h2 id=\"introduction-of-the-privacy-problem-in-machine-learning\"><a class=\"markdownIt-Anchor\" href=\"#introduction-of-the-privacy-problem-in-machine-learning\">#</a> Introduction of the Privacy Problem in Machine Learning</h2>\n<p>Machine learning is the foundation of popular Internet services such as image and speech recognition and natural language translation and recommend system, with many companies offering machine learning services via APIs (e.g., Google and Amazon).</p>\n<p>However, these models often learn sensitive user data during training, leading to potential privacy risks. Specifically, machine learning models can inadvertently leak their training data, posing a threat to user privacy. This paper focuses on “membership inference attacks,” where an attacker can infer whether a specific data point was part of the model’s training set based on its outputs.</p>\n<h2 id=\"machine-learning-background\"><a class=\"markdownIt-Anchor\" href=\"#machine-learning-background\">#</a> Machine Learning Background</h2>\n<p>Machine learning is categorized into supervised and unsupervised learning: Supervised learning uses labeled data to train models to predict outputs from inputs.</p>\n<p>A common issue in machine learning is overfitting, where the model performs well on training data but poorly on unseen data. Overfitted models are more likely to memorize training data, leading to privacy leaks, as they retain too much information about the data they trained on.</p>\n<p>Well-regularized models should avoid overfitting, generalizing well to new data without revealing sensitive information from the training data.</p>\n<h2 id=\"privacy-in-machine-learning\"><a class=\"markdownIt-Anchor\" href=\"#privacy-in-machine-learning\">#</a> Privacy in Machine Learning</h2>\n<p>Machine learning models can unintentionally leak sensitive information about the data they were trained on.</p>\n<p>Two primary types of privacy risks are:</p>\n<p><strong>Population-level inference</strong>: Inferring general patterns about the population used to train the model, which can reveal sensitive characteristics.</p>\n<p><strong>Membership inference</strong>: Determining if a specific individual’s data was included in the training set.</p>\n<p>This paper focuses on membership inference attacks, as protecting the privacy of training set members is both practical and critical for users.</p>\n<p>The risk is higher for models trained on private or sensitive data, such as healthcare records.</p>\n<h2 id=\"problem-statements\"><a class=\"markdownIt-Anchor\" href=\"#problem-statements\">#</a> Problem Statements</h2>\n<p>The core problem studied in this paper is membership inference attacks.<br>\nIn a black-box setting, the attacker can query the model to get outputs but does not have access to the model’s structure or parameters.<br>\nThe attacker’s goal is to determine if a specific data point was part of the model’s training set based on the model’s output.<br>\nThe paper assumes that the attacker might have some background knowledge, such as understanding the input format or statistical distribution of the dataset.<br>\nThe attack relies on detecting subtle differences in how the model behaves with data it has seen before (training data) versus new data (non-training data).</p>\n<h1 id=\"methods\"><a class=\"markdownIt-Anchor\" href=\"#methods\">#</a> Methods</h1>\n<h2 id=\"main-steps\"><a class=\"markdownIt-Anchor\" href=\"#main-steps\">#</a> Main steps:</h2>\n<p>The attacker queries the target model with a data record and obtains the model’s prediction on that record. The prediction is a vector of probabilities, one per class, that the record belongs to a certain class. This prediction vector, along with the label of the target record, is passed to the attack model, which infers whether the record was in or out of the target model’s training dataset.</p>\n<p><img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/Main_step.png\" alt=\"Main steps of MIA\"></p>\n<h2 id=\"shadow-models\"><a class=\"markdownIt-Anchor\" href=\"#shadow-models\">#</a> Shadow Models</h2>\n<p>The attacker uses the input and output data from shadow models to train the attack model. Specifically, the prediction results from the shadow models (confidence vectors or other outputs) are used as training data to train a binary classifier that can predict whether a particular data point was part of the shadow model’s training set.</p>\n<p>Since the behavior of the shadow model is similar to that of the target model, the attack model can learn from the shadow model’s training to infer which data points were used in the training of the target model.</p>\n<p><img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/Shadow_Models.png\" alt=\"Shadow Models\"></p>\n<h2 id=\"training-the-attack-model\"><a class=\"markdownIt-Anchor\" href=\"#training-the-attack-model\">#</a> Training the Attack Model</h2>\n<p>The inputs and outputs of the shadow models are used to train the attack model：</p>\n<p>The attack model is a binary classifier that learns to distinguish between “training data” (members) and“non-training data” (non-members).</p>\n<p>It uses the prediction vectors from the shadow models to learn how to classify data points as members or non-members.</p>\n<p><img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/Training_attack_models.png\" alt=\"Main steps of training the attack models\"></p>\n<h1 id=\"experimental-evaluation\"><a class=\"markdownIt-Anchor\" href=\"#experimental-evaluation\">#</a> Experimental Evaluation</h1>\n<h2 id=\"datasets-and-target-models\"><a class=\"markdownIt-Anchor\" href=\"#datasets-and-target-models\">#</a> Datasets and Target Models</h2>\n<p>The datasets used for experiments are described, including the type, size, and nature of the data, includes: Public datasets such as image datasets (e.g., CIFAR-10), location datasets, and some sensitive data sets .</p>\n<p>In this paper, the authorevaluated our inference attacks on three types of target models: two constructed by cloud-based “machine learning as a service” platforms and one implemented locally. all the attacks treat the models as black boxes, which means they do not know the type or structure of the models they create, nor the values of the hyper-parameters used during the training process.</p>\n<p>The training set and the test set of each target and shadow model are randomly selected from the respective datasets, have the same size, and are disjoint. There is no overlap between the datasets of the target model and those of the shadow models, but the datasets used for different shadow models can overlap with each other.</p>\n<h2 id=\"accuracy-of-the-attack\"><a class=\"markdownIt-Anchor\" href=\"#accuracy-of-the-attack\">#</a> Accuracy of the attack</h2>\n<p>This paper evaluate the attack by executing it on randomly reshuffled records from the target’s training and test datasets. And use the standard precision and recall metric to evaluate the percision.</p>\n<p>The test accuracy of the target neural-network models with the largest training datasets is low, which means the models are heavily overfitted on their training sets.</p>\n<p>For different deep learning APIs from different companies, the paper trained the same datas for the models and make the evaluation attacks, the result shows that Models trained using Google Prediction API exhibit the biggest leakage.</p>\n<p>For the Texas hospital-stay dataset and location adtaset, the paper evaluated the attack against a Google-trained model.</p>\n<p>The training accuracy of the Texas’s target model is 0.66 and its test accuracy is 0.51. Precision is mostly above 0.6, and for half of the classes, it is above 0.7. Precision is above 0.85 for more than 20 classes.</p>\n<p>The training accuracy of the location’s target model is 1 and its test accuracy is 0.66.Precision is between 0.6 and 0.8, with an almost constant recall of 1.<br>\nThe attacks against the google trained models for location data sets shows that the paper’s attacks are robust even if the attacker’s assumptions about the distribution of the target model’s training data are not very accurate.</p>\n<p>For the majority of the target model’s classes, the paper’s attack achieves high precision. This demonstrates that a membership inference attack can be trained with only black-box access to the target model, without any prior knowledge about the distribution of the target model’s training data if the attacker can efficiently generate inputs that are classified by the target model with high confidence.</p>\n<h1 id=\"factors-and-defenses\"><a class=\"markdownIt-Anchor\" href=\"#factors-and-defenses\">#</a> Factors and Defenses</h1>\n<h2 id=\"factors-for-success-of-membership-inference\"><a class=\"markdownIt-Anchor\" href=\"#factors-for-success-of-membership-inference\">#</a> Factors for Success of membership inference</h2>\n<p>Based on the evaluation, this paper bring forward two factors for a success membership inference attack:</p>\n<p><strong>generalizability of the target model</strong></p>\n<p><strong>diversity of its training data</strong></p>\n<p>If the model overfits and does not generalize well to inputs beyond its training data, or if the training data is not representative, the model leaks information about its training inputs.</p>\n<p>This paper also point out that overfitting is not the only reason why our inference attacks work. Different machine learning models, due to their different structures, “remember” different amounts of information about their training datasets. This leads to different amounts of information leakage even if the models are overfitted to the same degree</p>\n<h2 id=\"mitigation-strategies\"><a class=\"markdownIt-Anchor\" href=\"#mitigation-strategies\">#</a> Mitigation strategies</h2>\n<p>This paper indicates some strategies of mintigate the membership interface of the models:</p>\n<ol>\n<li>Restrict the prediction vector to top k classes.</li>\n<li>Coarsen（变粗糙） precision of the prediction vector.</li>\n<li>Increase entropy of the prediction vector.</li>\n<li>Use regularization.</li>\n</ol>\n<p>However, in this paper, they make evaluation about these mitigation strategies, and find that their attack method is still robust against these mitigation strategies</p>\n<h1 id=\"conclusion\"><a class=\"markdownIt-Anchor\" href=\"#conclusion\">#</a> Conclusion</h1>\n<p>This paper have designed, implemented, and evaluated the first membership inference attack against machine learning models, notably black-box models trained in the commerical deep learning APIs.</p>\n<p>This paper’s key technical innovation is the shadow training tech- nique that trains an attack model to distinguish the target model’s outputs on members versus non-members of its train- ing dataset.</p>\n<p>Membership in hospital-stay and other health-care datasets is sensitive from the privacy perspective. Therefore, this method may have substantial practical privacy implications.</p>\n",
            "tags": []
        },
        {
            "id": "https://ckti.github.io/2024/09/28/Introduction%20of%20MIA/",
            "url": "https://ckti.github.io/2024/09/28/Introduction%20of%20MIA/",
            "title": "Introduction of Membership Interface Attack",
            "date_published": "2024-09-28T13:52:47.000Z",
            "content_html": "<p>This is the English version of <a href=\"/public/2024/08/02/Introduction%20of%20MIA_CN/index.html\">this</a> blog. The translation may be not correct, if there are any mistakes, please contact me for delivering suggestions.</p>\n<h1 id=\"prelimaries-defination-and-classification\"><a class=\"markdownIt-Anchor\" href=\"#prelimaries-defination-and-classification\">#</a> Prelimaries ,Defination and classification</h1>\n<h2 id=\"prelimaries-machine-learning\"><a class=\"markdownIt-Anchor\" href=\"#prelimaries-machine-learning\">#</a> Prelimaries-- Machine learning</h2>\n<p>Machine learning (ML) is the study of computer algorithms that improve automatically through experience and by learning from data. Generally, it can be  be divided into these two categories</p>\n<p><strong>Supervised Learning</strong> :Uses labeled data where input-output pairs are known to train models. It aims to predict or classify new data based on learned patterns, minimizing the error between predicted and actual outcomes.</p>\n<p><strong>Unsupervised Learning</strong> :Works with unlabeled data to identify hidden patterns or structures. It clusters or finds relationships within the data without predefined labels.</p>\n<h2 id=\"membership-interface-attackdefination\"><a class=\"markdownIt-Anchor\" href=\"#membership-interface-attackdefination\">#</a> Membership Interface Attack–Defination</h2>\n<p>Membership interface attack (MIA) is a method of obtaining the data being trained in the targeted ML models byThe main thought of MIA is forming a models that can repersents the membership of the target models. training similar model called shadow model and training a classifier to check whether a data is being trained in the targeted ML model.</p>\n<h3 id=\"shadow-models\"><a class=\"markdownIt-Anchor\" href=\"#shadow-models\">#</a> Shadow models</h3>\n<p>The main thought of MIA is forming a models that can repersents the membership of the target models.</p>\n<p><img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/Shadow%20model.png\" alt=\"shadow models and training\"></p>\n<h2 id=\"basic-taxonomy-of-mia\"><a class=\"markdownIt-Anchor\" href=\"#basic-taxonomy-of-mia\">#</a> Basic Taxonomy of MIA</h2>\n<h3 id=\"based-on-informations\"><a class=\"markdownIt-Anchor\" href=\"#based-on-informations\">#</a> based on informations</h3>\n<p>Based on the informations of the target model attackers can get, the MIA can be divided into these two categories:</p>\n<p><strong>White-box attack</strong>: Attackers can obtain all the information about the ML model, including the data distributions, training methods and relevant  parameters</p>\n<p><strong>Black-box attack</strong>: Attackers can only get restricted informations, including restricted data distributions, training methods and parameters.</p>\n<p>Compared with white-box attack, black-box attacks gets fewer informations, which leads it harder to achieve. However, the influence of a successful attacks can be  larger than it. Today , the mainstream of the research is also focusing on the black-box attacks.</p>\n<h3 id=\"based-on-prediction-vectors\"><a class=\"markdownIt-Anchor\" href=\"#based-on-prediction-vectors\">#</a> based on Prediction vectors</h3>\n<p>The prediction vectors is a parameter for judging whether a data is in the models. Today, the main research of MIA can also be categorized based on prediction vectors in the following graph:</p>\n<p><img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/Prediction%20vectors.png\" alt=\"Taxonomy of Prediction vectors\"></p>\n<h3 id=\"based-on-the-judgement-methods\"><a class=\"markdownIt-Anchor\" href=\"#based-on-the-judgement-methods\">#</a> Based on the judgement methods</h3>\n<h4 id=\"binary-classifier-based-mia\"><a class=\"markdownIt-Anchor\" href=\"#binary-classifier-based-mia\">#</a> Binary Classifier Based MIA.</h4>\n<p>The data can be classified as members or non-members by training a binary classifier for classify the models. the main method is as followed:</p>\n<p><strong>Training the shadow models</strong>: Attackers use multiple shadow models that have the same or similar distributions of the target training sets to learn and training the shadow models.</p>\n<p><strong>Collecting prediction models</strong>: Attackers will make search for the shadow training sets and get the prediction vectors of each data records. Each data of the vectors in shadow training data sets can be labeled as “Member” and the data in test data sets are labled “Non-member”</p>\n<p><strong>Training the attack models</strong>: Form the “member”and “non-member”data sets based on the labeled datas , and training a binary classifier to form it.</p>\n<p>By this, Identifying the complex problem of recognizing the member and non-member of the model is converted into the binary classifier problem.</p>\n<h4 id=\"metric-based-membership-inference-attacks\"><a class=\"markdownIt-Anchor\" href=\"#metric-based-membership-inference-attacks\">#</a> Metric Based Membership Inference Attacks</h4>\n<p>Metric Based MIA obtain the relative metrics by collecting and analysing the prediction vectors and make analyzations by comparing the metrics and thresold values.</p>\n<p>Compare with training the binary classifier, it is more simple and consume less computational resources. The next page will show the recent research realms of the analyse and settings.</p>\n<p>Currently, the research on metric based MIA have these categories:</p>\n<p><strong>Prediction Correctness Based MIA</strong>: If the target models predict the input data x correctly, then attackers recognized it as a member. The inituation of it is that if a data is in the real data, then the target model will predict the input data x correctly.</p>\n<p><strong>Loss rate Based MIA</strong>: If the difference of the loss rate correspond to the target models and the lossrate of the origin data is less than a thresold, then attackers recognize it as a member. The inituation of it is that if the input data is in the true datas, then the loss rate of the target models is near to the total loss rate.</p>\n<p><strong>Prediction Confidence Based MIA</strong>: If the prediction confidence of some records is larger than some thresold, then recognize it as a member. The inituation of it is that the target models will minimize the difference between it and the real models, so the prediction confidence will close to 1.</p>\n<p><strong>Prediction Entropy Based MIA</strong>: If the prediction entropy of input a records is lower than a thresold, then recognize it as a member. The inituation is that the target model’s prediction entropy of the training data is larger than the prediction of entropy of the test data.</p>\n<p><strong>Modified prediction Entropy Based MIA</strong>: Some opinion suggest that current prediction loss didn’t think of the ground truth label, so it may make some misjudgement of some datas, so in some papers, the algorithm of the prediction entropy is modified.</p>\n<h1 id=\"relavant-research\"><a class=\"markdownIt-Anchor\" href=\"#relavant-research\">#</a> Relavant research</h1>\n<h2 id=\"research-on-the-classification-models\"><a class=\"markdownIt-Anchor\" href=\"#research-on-the-classification-models\">#</a> Research on the classification models</h2>\n<p>Since Shokri et al. introduced this attack method, there has been a growing body of research focused on this direction. Salem et al. discussed the assumptions of Membership Inference Attacks (MIA) and attempted to relax the implementation conditions, demonstrating that two of the shadow model assumptions are not necessary and proposing an indicator-based MIA approach. Yeom et al. also proposed two indicator-based MIA methods; Long et al. achieved MIA attacks on certain data by focusing on data with unique effects on the target model, enabling accurate inference in generalized models with similar training and testing accuracy.</p>\n<p>Additionally, existing research has also targeted more restricted MIAs. Li and Zhang proposed transfer-based and perturbation-based MIAs. Transfer-based MIAs construct shadow models to simulate the target model, using the shadow model’s confidence to determine membership; perturbation-based MIAs introduce noise to create adversarial examples and distinguish members based on the severity of the perturbation. Choquette et al. introduced data augmentation-based MIAs and decision boundary distance-based MIAs. Data augmentation attacks target common data augmentation phenomena in machine learning systems, creating additional records through different augmentation strategies to query the target model for predictions. Decision boundary attacks estimate the distance of records to the model boundary, similar to Li and Zhang’s attacks. Successful MIA cases suggest that machine learning models may be more vulnerable to MIA than previously anticipated.</p>\n<p>Aside from black-box MIA attacks, Nasir et al. introduced white-box MIA, which can be seen as an extension of black-box MIA, enhancing attack efficiency by leveraging additional information. They use the gradient of the target model’s prediction loss for inference and train a model to distinguish between members and non-members using the SGD algorithm. However, Leino and Fredrikson pointed out that the assumptions of this method are too stringent, requiring attackers to know the approximate distribution of the target dataset. They proposed a Bayes-optimal attack-based MIA method, enabling MIA without background knowledge of the target model.</p>\n<h2 id=\"research-on-generative-models\"><a class=\"markdownIt-Anchor\" href=\"#research-on-generative-models\">#</a> Research on Generative Models</h2>\n<p>Corrent research on generative models are focusing on the generative adversarial network, whose models can be shown as follows:</p>\n<p><img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/GAN_architecture.png\" alt=\"\"></p>\n<p>Hayes et al. were the first to propose Membership Inference Attacks (MIA) against generative models. For white-box attacks, attackers collect all records and compute confidence scores to make inferences; for black-box attacks, attackers collect records from the generator to train a local GAN that mimics the target GAN, and then use the local GAN discriminator for inference. Hilprecht et al. introduced two additional attacks: a Monte Carlo-based black-box attack and a VAE-based white-box attack. Hilprecht et al. proposed a set-based attack to determine whether a data point belongs to a set, while Liu et al. introduced a similar co-membership inference attack, determining dataset membership by analyzing the distance of a data point to the target data. Chen et al. proposed a general method where attackers continuously reconstruct the attack model through optimization, calculate the distance between the generated results of the attack model and the target model, and estimate the probability of data membership based on this distance.</p>\n<h2 id=\"research-on-embedding-models\"><a class=\"markdownIt-Anchor\" href=\"#research-on-embedding-models\">#</a> Research on Embedding Models：</h2>\n<p>Current research primarily focuses on text and image embedding models. For text embedding models, attacks aim to infer membership of words or sentence pairs within a sliding window, using similarity scores to determine if they belong to a predefined set. For graph embedding models, attack methods involve using shadow models and confidence scores to infer whether nodes in the graph belong to specific categories, addressing node classification issues.</p>\n<h2 id=\"research-on-regression-models\"><a class=\"markdownIt-Anchor\" href=\"#research-on-regression-models\">#</a> Research on Regression Models：</h2>\n<p>Gupta et al. were the first to conduct MIA (Membership Inference Attack) research on regression models for age prediction, achieving attacks through the construction of a white-box binary classification model.</p>\n<h2 id=\"research-in-federated-learning\"><a class=\"markdownIt-Anchor\" href=\"#research-in-federated-learning\">#</a> Research in Federated Learning：</h2>\n<p>In federated learning, attackers can be either the central server or some of the participating clients. They can implement MIA by determining whether certain data was used in training the global model. Melis was the first to propose a gradient-based MIA by analyzing the update mechanism of the RNN training embeddings. Turex introduced heterogeneous FL (Federated Learning), which involves analyzing differences in aggregated parameters from different clients. Nasr et al. discussed how gradient ascent attacks can actively interfere with FL training. Hu et al. proposed source inference attacks aimed at determining which participants hold training records in FL. They argue that existing MIA attacks in FL overlook the source information of training members, which could lead to further privacy issues.</p>\n<h1 id=\"factors-for-a-success-mia\"><a class=\"markdownIt-Anchor\" href=\"#factors-for-a-success-mia\">#</a> Factors for a success MIA</h1>\n<h2 id=\"overfitting-of-target-models\"><a class=\"markdownIt-Anchor\" href=\"#overfitting-of-target-models\">#</a> Overfitting of Target Models</h2>\n<p>Many studies have pointed out that overfitting of target ML models is a significant factor in the leakage of original datasets. Specifically:Models like DNNs, due to their high parameterization in applications, enhance their ability to handle large datasets but also record a lot of irrelevant information.Training machine learning models often requires many epochs, making them more prone to memorizing the content of the dataset.Machine learning datasets cannot fully represent real-world data.Existing articles indicate that for a classification system overfitting on training data, attackers can achieve an attack success probability higher than 50% based on the correctness of randomly guessed predictions.</p>\n<h2 id=\"features-of-the-model-itself\"><a class=\"markdownIt-Anchor\" href=\"#features-of-the-model-itself\">#</a> Features of the Model Itself：</h2>\n<p>When the decision boundary of the target model is not sensitive to the training data used, the effectiveness of MIA attacks is low. Current research shows that among DNN models, logistic regression models, Naive Bayes models, k-nearest neighbor models, and decision tree models, decision tree models have the highest attack accuracy, while the simple Naive Bayes algorithm has the lowest.</p>\n<h2 id=\"diversity-of-the-training-dataset\"><a class=\"markdownIt-Anchor\" href=\"#diversity-of-the-training-dataset\">#</a> Diversity of the Training Dataset:</h2>\n<p>When the training dataset used by the target model is highly diverse, it helps the model generalize better to test data. Consequently, the impact of MIA on the model will be smaller.</p>\n<h2 id=\"attackers-knowledge-of-the-target-model\"><a class=\"markdownIt-Anchor\" href=\"#attackers-knowledge-of-the-target-model\">#</a> Attacker’s Knowledge of the Target Model:</h2>\n<p>Existing research on MIA generally makes certain assumptions about the attacker: the attacker knows the relevant distribution of the training data and can construct a suitable shadow dataset based on this distribution. High-accuracy shadow models constructed under this assumption are needed for effective attacks.</p>\n<h1 id=\"research-on-defense-against-mia\"><a class=\"markdownIt-Anchor\" href=\"#research-on-defense-against-mia\">#</a> Research on Defense against MIA</h1>\n<h2 id=\"confidence-score-masking\"><a class=\"markdownIt-Anchor\" href=\"#confidence-score-masking\">#</a> Confidence Score Masking</h2>\n<p>This method is mainly used for defending against black-box attacks by returning obfuscated true confidence scores to the classifier. It includes the following three approaches:</p>\n<ol>\n<li>The target classifier does not provide the full prediction vector but only the top few confidence scores.</li>\n<li>The target classifier only provides predicted labels when the attacker provides data input.</li>\n<li>Noise is added to the returned vector.</li>\n</ol>\n<p>These three methods affect the prediction vector but do not result in a loss of prediction accuracy.</p>\n<h2 id=\"regularization\"><a class=\"markdownIt-Anchor\" href=\"#regularization\">#</a> Regularization</h2>\n<p>Regularization mitigates MIA attack strength by reducing model overfitting.</p>\n<p>Existing regularization methods include traditional techniques such as L2-norm regularization, dropout, data augmentation, model stacking, early stopping, and label smoothing. These methods lower overfitting by reducing the impact of different test datasets on samples, thereby also reducing the intensity of MIA attacks.</p>\n<p>Additionally, specially designed regularization systems like adversarial regularization and Mixup + MMD (Maximum Mean Discrepancy) can also defend against MIA by introducing new regularization mechanisms to decrease the differences between members and non-members. Compared to masking techniques, regularization can resist both black-box and white-box attacks and can alter output parameters when modifying the output model.</p>\n<h2 id=\"knowledge-distallation\"><a class=\"markdownIt-Anchor\" href=\"#knowledge-distallation\">#</a> Knowledge Distallation</h2>\n<p>Knowledge distillation refers to the process of training a smaller student model using a larger teacher model to transfer knowledge from the large model to the small one, allowing the smaller model to achieve a similar level of approximation. Based on this, existing research has introduced methods such as DMP, CKD, and PCKD, they are generally called DMP (Distillation For Membership Privacy), whose steps are as followed:</p>\n<ol>\n<li>Train an unprotected teacher model to record and label data in an unlabeled dataset.</li>\n<li>Select data with lower prediction entropy for training, which is used for classification.</li>\n<li>Train based on the labeled model.</li>\n</ol>\n<p>Additionally, There are also some research that proposes Complementary Knowledge Distillation (CKD) and Pseudo Complementary Knowledge Distillation (PCKD) methods. In these methods, the transfer data for knowledge distillation comes from a private training set. CKD and PCKD eliminate the need for public data, which may be difficult to obtain in some applications, making knowledge distillation a more practical defense method for mitigating MIA attacks on machine learning models.</p>\n<h2 id=\"differencial-privacy\"><a class=\"markdownIt-Anchor\" href=\"#differencial-privacy\">#</a> Differencial Privacy</h2>\n<p>Differential privacy refers to protecting the original data by adding relevant noise to the dataset. When a deep learning model is trained with a model that incorporates differential privacy, if the privacy budget is small enough, the trained model will not retain specific user information. Therefore, different privacy models can limit the success rate of MIA attacks based solely on the model. Current research realms are shown in the next page:</p>\n<p>Differential privacy provides theoretical protection for member privacy in training records and can mitigate MIAs in classification and generative models, regardless of whether the attacker is in a black-box or white-box setting. Despite its widespread and effective application, a drawback is its difficulty in providing an acceptable privacy-utility tradeoff in complex learning tasks. Additionally, differential privacy can also mitigate other forms of privacy attacks, such as attribute inference attacks and feature inference attacks, and is related to the robustness of models against adversarial examples.</p>\n<p>Currently , the research realms about differencial privacy is:</p>\n<p><strong>The relationship between differential privacy and MIA</strong>: There are theoretical results and proofs on this, but practical evaluations have not achieved good utility.</p>\n<p><strong>Privacy-Utility Tradeoff</strong>: Existing research shows that current differential privacy performance in this regard is insufficient. Studies indicate that minority groups are more affected by MIAs, and differential privacy reduces model utility for these groups.</p>\n<p><strong>Training Methods</strong>: Current methods primarily include DP-SGD, with new methods like DP-Logits also being proposed.</p>\n<p><strong>Applications in Generative Models</strong>: Research shows that differential privacy can also defend against MIAs in generative models, with defense effectiveness related to generative quality and privacy budget 𝜖. Studies indicate that differential privacy limits overfitting and mitigates MIA.</p>\n<h1 id=\"possible-future-directions\"><a class=\"markdownIt-Anchor\" href=\"#possible-future-directions\">#</a> Possible future directions</h1>\n<h2 id=\"in-membership-inference-attacks\"><a class=\"markdownIt-Anchor\" href=\"#in-membership-inference-attacks\">#</a> In Membership Inference Attacks</h2>\n<p><strong>Attacks on Regularized Models</strong>: MIA systems often rely on the overfitting of machine learning systems, but this assumption is challenged by advancements in regularization techniques; attacks on overfitted models are still largely unexplored.</p>\n<p><strong>Attacks on Self-Supervised Models</strong>: Self-supervised models are becoming widespread in NLP and computer vision, and attacks on these models are still largely unknown.</p>\n<p><strong>Attacks on Adversarial Machine Learning</strong>: Adversarial machine learning shares some similarities and differences with membership inference attacks; combining these approaches could be a potential research direction.</p>\n<p><strong>Attacks on New Machine Learning Models</strong> (e.g., Contrastive Learning and Meta-Learning): These models differ significantly from traditional ones, and many areas for research remain in attacking them.</p>\n<p><strong>Attacks on Federated Learning</strong>: Existing MIAs are mainly applicable to homogeneous federated learning, with limited research on heterogeneous federated learning.</p>\n<p><strong>Applications Related to MIA</strong>: Includes source inference attacks in federated learning and deeper privacy protection studies through MIA audits of data contributions to ML models.</p>\n<h2 id=\"in-the-defence-of-membership-inference-attacks\"><a class=\"markdownIt-Anchor\" href=\"#in-the-defence-of-membership-inference-attacks\">#</a> In the Defence of Membership Inference Attacks</h2>\n<p><strong>Defense Against Unsupervised Learning Models</strong>: Unsupervised learning models struggle with overfitting due to a lack of data labels, and research in this area is limited.</p>\n<p><strong>Defense Against Generative Models</strong>: Possible defenses include methods such as knowledge distillation and reinforcemnt learning to avoid leakage of raw data through the outputs of generative models.</p>\n<p><strong>Balancing Privacy and Utility</strong>: Existing differential privacy protections often add significant noise to classifier gradients, reducing prediction accuracy. Balancing privacy and utility remains an area for research.</p>\n<p><strong>Privacy Defenses in Federated Learning</strong>: With increasing privacy attacks in federated learning, developing defensive technologies is crucial, with differential privacy being a potential future direction.</p>\n",
            "tags": []
        },
        {
            "id": "https://ckti.github.io/2024/09/28/Introduction%20of%20MIA_CN/",
            "url": "https://ckti.github.io/2024/09/28/Introduction%20of%20MIA_CN/",
            "title": "成员推理攻击介绍",
            "date_published": "2024-09-28T13:52:41.000Z",
            "content_html": "<p><a href=\"/https://arxiv.org/abs/2103.07853\">论文地址</a><br>\n标题: Membership Inference Attacks on Machine Learning: A Survey</p>\n<h1 id=\"mia-的前置知识和定义\"><a class=\"markdownIt-Anchor\" href=\"#mia-的前置知识和定义\">#</a> MIA 的前置知识和定义</h1>\n<h2 id=\"定义\"><a class=\"markdownIt-Anchor\" href=\"#定义\">#</a> 定义</h2>\n<p>成员推理攻击 (Membership Inference Attacks) 是指对一个人工智能系统，通过猜测其中的数据并训练一个 shadow model 猜测对应数据是否在其中。</p>\n<h2 id=\"基本分类\"><a class=\"markdownIt-Anchor\" href=\"#基本分类\">#</a> 基本分类</h2>\n<p>针对 MIA 的目标模型性质，可将 MIA 攻击分为以下两种，具体可以参见下图:<br>\n<img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/B%26W_attacks.png\" alt=\"两种攻击的差异\"></p>\n<p><strong>白盒攻击</strong>：攻击者可以获取模型的所有信息，包括数据分布，训练方式以及相关变量<br>\n<strong>黑盒攻击</strong>：攻击者只能进行黑盒访问，获取受限的数据分布，训练方式等中间信息</p>\n<p>相较于白盒攻击，黑盒攻击由于可获取的信息较少，因此实现难度较大，但攻击成功造成的影响也更大，现今主要的研究方向也集中于黑盒攻击，根据提供的预测向量，可以分为：Full confidence scores 、Top-K confidence scores、Prediction label only 三种方式，具体介绍见下图：<br>\n<img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/Prediction%20vectors.png\" alt=\"预测向量\"></p>\n<h1 id=\"mia的攻击特点和相关研究\"><a class=\"markdownIt-Anchor\" href=\"#mia的攻击特点和相关研究\">#</a> MIA 的攻击特点和相关研究</h1>\n<h2 id=\"攻击模型的特点\"><a class=\"markdownIt-Anchor\" href=\"#攻击模型的特点\">#</a> 攻击模型的特点</h2>\n<p>机器学习模型通常会造成过拟合，这种过拟合为 MIA 提供了便利，根据所使用的攻击模型，可以将 MIA 分为以下两类：</p>\n<h3 id=\"基于二元分类器的mia-binary-classifier-based-membership-inference-attacks\"><a class=\"markdownIt-Anchor\" href=\"#基于二元分类器的mia-binary-classifier-based-membership-inference-attacks\">#</a> 基于二元分类器的 MIA （Binary Classifier Based Membership Inference Attacks.）</h3>\n<p>通过训练二元分类器的 MIA 可以将目标模型的训练模型与非成员区分开来，在现有的研究中，Shokri 等人提出了一种称为影子训练 (<a href=\"/https://arxiv.org/abs/1610.05820\">shadow traing</a>) 的有效技术，其表示如下图所示：<br>\n<img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/Shadow%20model.png\" alt=\"影子训练的图示\"></p>\n<p>攻击者首先使用影子训练数据集和学习算法来训练这些影子模型，使它们的行为尽可能接近目标模型。具体的训练方法如下：</p>\n<p>1.<strong> 训练影子模型</strong>：攻击者使用多个与目标训练集分布相同影子训练数据集和学习算法训练影子模型<br>\n 2.<strong> 收集预测向量</strong>：攻击者会使用影子训练数据集和影子测试数据集对影子模型进行查询，获取每条数据记录的预测向量。影子训练数据集的每条记录的预测向量被标记为 “成员”，影子测试数据集的每条记录的预测向量被标记为 “非成员”。<br>\n3.<strong> 构建训练数据集</strong>：根据第二部标记的数据构造 “成员” 和 “非成员” 的数据集<br>\n 4.<strong> 训练攻击模型</strong>：识别训练数据集成员和非成员的复杂关系问题被转换为二分类问题，并通过机器学习框架进行学习。</p>\n<p>现有的对于二元分类器的研究同样分为黑盒攻击和白盒攻击，具体可参见开头提到的文章，主要通过训练分类函数以此计算损失率。二者区别大致可参见下图：<br>\n<img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/Binary_classifier.png\" alt=\"\"></p>\n<h3 id=\"基于指标的mia-metric-based-membership-inference-attacks\"><a class=\"markdownIt-Anchor\" href=\"#基于指标的mia-metric-based-membership-inference-attacks\">#</a> 基于指标的 MIA (Metric Based Membership Inference Attacks)</h3>\n<p>基于指标的 MIA 通过对预测向量进行收集分析并获取相关的指标，并通过比对指标和阈值进行分析，相比训练二元分类器，其更为简单并且消耗较少的计算资源，现今对指标的分析以及设定主要基于以下方面：</p>\n<p>1.<strong> 基于预测正确性的攻击 (Prediction Correctness Based MIA)</strong>：如果目标模型正确预测了输入记录 x，那么攻击者则判断其为成员，否则将其判断为非成员。从直觉上看，输入数据如果在真实数据中，那么目标模型就回正确预测输入记录 x。<br>\n2.<strong> 基于预测损失的攻击 (Prediction Correctness Based MIA)</strong>：如果输入某个记录对应的损失率与原始数据差距小于某个阈值，那么就判断其为成员，反之则否。从直觉上看，输入数据如果在真实数据中，那么目标模型的损失率就应该在整体的损失率附近<br>\n 3.<strong> 基于预测置信度的攻击 (Prediction Confidence Based MIA)</strong>：如果输入某个记录后的预测置信度大于某个阈值，则判断其为成员。从直觉上看，目标模型将最小化其与实际模型的差距，因此其预测置信度应当接近 1。<br>\n4.<strong> 基于预测熵的攻击（Prediction Entropy Based MIA）</strong> 如果输入某个记录后其预测熵小于某个阈值，则判断其为成员。从直觉上看，目标模型的对试验预测熵都会大于其对测试的预测熵<br>\n 5.<strong> 基于修改后的预测熵的攻击（Modified Prediction Entropy Based MIA）</strong>：有观点认为现有的预测熵未考虑 ground truth label，因而可能造成某些数据被误判。因此在<span class=\"exturl\" data-url=\"aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDMuMTA1OTU=\">这篇文章</span>中作者对预测熵的算法进行了一些修改</p>\n<h2 id=\"根据目标模型的相关研究\"><a class=\"markdownIt-Anchor\" href=\"#根据目标模型的相关研究\">#</a> 根据目标模型的相关研究</h2>\n<h3 id=\"在分类模型方面的研究\"><a class=\"markdownIt-Anchor\" href=\"#在分类模型方面的研究\">#</a> 在分类模型方面的研究</h3>\n<p>自 Shokri 等人提出了这一攻击方式后，目前有许多针对此方向的研究。Salem 等人讨论了 MIA 的假设并尝试放宽了实现的条件，证明了 shadow model 的两种假设并非必须并提出了基于指标的 MIA 方式。Yeom 等人也提出了两种基于指标的 MIA 方式；Long 等人通过调关注某些对目标模型有独特影响的数据，以此实现对某些数据的 MIA 攻击，实现了在训练和测试准确度相近的情况下，在通用化的模型中正确进行推断。</p>\n<p>此外，现有的研究同样也针对更加受限的 MIA，Li 和 Zhang 提出了基于基于转移的（transfer based）MIA 和基于扰动（perturbation based）的 MIA。基于转移的 MIA 通过构造影子模型模拟目标模型，用阴影模型的置信度判断成员；基于扰动的 MIA 则通过添加噪音，使其变成对抗性的例子，通过扰动的严重程度区分成员。Choquette 等人还提出了基于数据增强的 MIA 和基于决策边界距离的 MIA。基于数据增强的攻击针对机器学习系统中常见的数据增强现象，通过不同的数据增强策略创建额外的数据记录，以此查询目标模型收集预测标签。基于决策边界的攻击策略则估计记录到模型边界的距离，其类似 Li 和 zhang 的攻击。现有的 MIA 成功案例表明，机器学习模型可能比我们预期的更容易收到 MIA 的影响。</p>\n<p>除去针对 MIA 的黑盒攻击，Nasir 等人首次提出了白盒 MIA，其可看作是基于黑盒 MIA 在进行的的拓展，通过获取的更多信息提升攻击效率。他们采用目标模型预测损失的梯度进行推断，以此通过 SGD 算法训练区分成员与非成员。但 Leino and Fredrikson 指出其方法的假设过于严格，其需要攻击者知道目标数据集的大致分布，因此他们提出了一种基于贝叶斯最佳攻击（Bayes-optimal attack）的 MIA 方法，从而实现无须目标模型背景知识的 MIA。</p>\n<h3 id=\"在生成模型上的研究\"><a class=\"markdownIt-Anchor\" href=\"#在生成模型上的研究\">#</a> 在生成模型上的研究</h3>\n<p>现有的研究主要集中于生成式对抗神经网络上 (generative adversarial network). 其模型大致如下图可见<br>\n<img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/GAN_architecture.png\" alt=\"\"></p>\n<p>Hayes 等人首次提出了关于生成模型的 MIA，对于白盒攻击，攻击者收集所有记录并计算置信度进行判别；对于黑盒攻击，攻击者从生成器中收集记录以训练本地 GAN 以模仿目标 GAN，并通过使用本地 GAN 鉴别器进行判别。Hilprecht 等人提出了另外两种攻击，分别是基于蒙特卡洛的黑盒攻击和基于 VAE 的白盒攻击。Hilpreche 等人提出了判断某个数据是否在集合内的集合攻击，Liu 等人则提出了与其相似的 co-membership inference，通过分析某个数据到目标数据的距离来判断是否在数据集内。Chen 等人提出了一种通用的方法，攻击者通过最优化方法不断重建攻击模型，并根据攻击模型计算其生成结果与目标模型的距离，并且通过距离估算数据在其中的概率。</p>\n<h3 id=\"在嵌入模型上的研究\"><a class=\"markdownIt-Anchor\" href=\"#在嵌入模型上的研究\">#</a> 在嵌入模型上的研究</h3>\n<p>现有的研究主要针对文本和图像的研究，针对文本嵌入模型，攻击的目标是推断滑动窗口的词语或句子对的成员资格，利用它们的相似性分数来推断它们是否属于某个预定义的集合。对于图嵌入模型，攻击方法涉及使用阴影模型和置信度分数来推断图中节点是否属于特定类别，即节点分类问题。</p>\n<h3 id=\"在回归模型上的研究\"><a class=\"markdownIt-Anchor\" href=\"#在回归模型上的研究\">#</a> 在回归模型上的研究</h3>\n<p>Gupta 等人首次进行了针对年龄预测的回归模型上的 MIA 研究，通过构造白盒的二元判断模型实现攻击。</p>\n<h3 id=\"在联邦学习的研究\"><a class=\"markdownIt-Anchor\" href=\"#在联邦学习的研究\">#</a> 在联邦学习的研究</h3>\n<p>在联邦学习中，攻击者可以是中心服务器或者其中的某些分机，通过判断某些数据是否用于全局模型的训练以实现 MIA。Melis 通过分析 RNN 训练师嵌入层的更新机制，首次提出了基于梯度的 MIA。Turex 则提出了异构 FL（heterogeneous FL），通过分析不同分机汇总参数的差异进行判断。Nasr 等人讨论了如何通过梯度上升攻击主动干预 FL 训练。Hu 等人则提出了源推断攻击，旨在确定哪个参与方拥有 FL 中的训练记录。他们认为现有的 FL 中的成员推断攻击忽视了训练成员的来源信息，而这些信息的泄露可能导致进一步的隐私问题。</p>\n<h1 id=\"mia-攻击成功的因素\"><a class=\"markdownIt-Anchor\" href=\"#mia-攻击成功的因素\">#</a> MIA 攻击成功的因素</h1>\n<h2 id=\"目标模型对原始数据集的过拟合\"><a class=\"markdownIt-Anchor\" href=\"#目标模型对原始数据集的过拟合\">#</a> 目标模型对原始数据集的过拟合</h2>\n<p>该综述表明，目前已有许多研究指出 target ML models 对目标的过拟合是造成原始数据集泄漏的重要因素，具体有如下原因：</p>\n<ol>\n<li>DNN 等模型在应用中的过参数化一方面提升了处理大数据的能力，另一方面也记录了大量数据的无效信息。</li>\n<li>在训练机器学习时通常需要较多 epoch，使其更倾向于记忆数据集中的内容</li>\n<li>机器学习的数据集无法完全代表实际数据集</li>\n</ol>\n<p>对于一个对训练数据过拟合的分类系统，攻击者可以基于随机猜测的预测正确性实现高于 50% 的攻击成功概率，这点的证明可以在<a href=\"/https://arxiv.org/abs/2009.05669\">这篇文章</a>可见</p>\n<h2 id=\"训练模型自身的特征\"><a class=\"markdownIt-Anchor\" href=\"#训练模型自身的特征\">#</a> 训练模型自身的特征</h2>\n<p>当目标模型的决策边界对于所使用的训练数据并不敏感的时候，MIA 攻击的有效性不高。<a href=\"/https://ieeexplore.ieee.org/document/8962136\">这篇文章</a>实验数据表明，在 DNN models, logistic regression models, Naive Bayes models, k-nearest neighbor models, and decision tree models 对决策树模型具有最高的攻击精度，而简单贝叶斯算法 (Naive Bayes) 具有最低的攻击精度</p>\n<h2 id=\"训练数据集的多样性\"><a class=\"markdownIt-Anchor\" href=\"#训练数据集的多样性\">#</a> 训练数据集的多样性</h2>\n<p>当目标模型使用的训练数据集具有较强的多样性的时候，训练数据将帮助模型更好的概括测试数据。因此 MIA 对该模型的影响就会越小。</p>\n<h2 id=\"攻击者对目标模型的了解程度\"><a class=\"markdownIt-Anchor\" href=\"#攻击者对目标模型的了解程度\">#</a> 攻击者对目标模型的了解程度</h2>\n<p>现有的针对 MIA 的研究基本对攻击者都有一定的假设： 攻击者知道训练数据的相关分布，并且可以根据相关的分布构造出适合的影子数据集。基于这一假设构造的高精确度的影子模型才可以有效地实现攻击。</p>\n<h1 id=\"mia-的防御研究\"><a class=\"markdownIt-Anchor\" href=\"#mia-的防御研究\">#</a> MIA 的防御研究</h1>\n<h2 id=\"可信得分掩码confidence-score-masking\"><a class=\"markdownIt-Anchor\" href=\"#可信得分掩码confidence-score-masking\">#</a> 可信得分掩码 Confidence Score Masking</h2>\n<p>此方法主要用于黑盒攻击的防御，通过向分类器返回隐藏后的真实的可信得分以实现防御，具体有以下三种方式：</p>\n<ol>\n<li>target classifier 并不提供完整的预测向量，而是只提供最高的几 confedence score</li>\n<li>target classifier 只在攻击者提供数据输入时提供预测的标签</li>\n<li>将噪声添加到返回向量上<br>\n此三种方法只影响预测向量而不会造成预测精确度的损失</li>\n</ol>\n<h2 id=\"正则化\"><a class=\"markdownIt-Anchor\" href=\"#正则化\">#</a> 正则化</h2>\n<p>正则化通过降低模型的过拟合程度以减轻 MIA 的攻击强度。现有的正则化模型包括以下几种：传统的正则化方式是：L2-norm regularization, dropout , data argumentation, model stacking, early stopping, label smoothing. 其通过降低不同测试数据集对样本的影响以降低过拟合程度，同时也可以减轻对 MIA 攻击的强度。此外，如 adversarial regular- ization , and Mixup + MMD (Maximum Mean Discrepancy) 这两种特别设计的正则化系统同样也可以防御 MIA，通过往目标分类器中添加新的正则化机制以降低成员和非成员之间的差异<br>\n相比于掩码技术，正则化可以抵抗黑盒和白盒攻击，其在修改输出模型的时候也可以改变输出的参数</p>\n<h2 id=\"知识蒸馏knowledge-distillation\"><a class=\"markdownIt-Anchor\" href=\"#知识蒸馏knowledge-distillation\">#</a> 知识蒸馏 (Knowledge Distillation)</h2>\n<p>知识蒸馏指通过大型教师模型训练小型学生模型，以此将知识从大模型中传输到小模型中去，使小模型能够获得相近的近似程度。基于此，现有的研究提供了 DMP 以及 CKD 和 PCKD 方法：</p>\n<p><a href=\"/https://arxiv.org/abs/1906.06589\">DMP（Distillation For Membership Privacy）方法：</a>通过一个私有的数据集和参数数据集进行防御，具体步骤如下:</p>\n<ol>\n<li>训练一个无保护的教师模型，并以此在未标签的数据集中进行记录并标记</li>\n<li>选取其中预测熵较低的数据进行训练，这些数据以为分类</li>\n<li>基于已标记的模型进行训练。</li>\n</ol>\n<p>此外，<a href=\"/https://www.sciencedirect.com/science/article/abs/pii/S0925231221006329\">这一篇论文</a>提出了互补知识蒸馏（Complementary Knowledge Distillation，CKD）和伪互补知识蒸馏（Pseudo Complementary Knowledge Distillation，PCKD）方法。在这些方法中，知识蒸馏的转移数据都来自私有训练集。CKD 和 PCKD 消除了在某些应用中可能难以获得的公共数据的需求，使知识蒸馏成为一种更实用的防御方法来减轻机器学习模型上的 MIA 攻击。</p>\n<h2 id=\"差分隐私differential-privacy\"><a class=\"markdownIt-Anchor\" href=\"#差分隐私differential-privacy\">#</a> 差分隐私 (Differential Privacy)</h2>\n<p>差分隐私指的是通过向原始数据集中添加相关的扰动数，以实现对原始数据的保护，当一个深度学习模型使用了差分隐私后的模型进行训练时，如果其隐私预算够小，那么学习后的模型并不会记住用户的相关信息。由此，不同的隐私模型就可以限制仅基于模型的 MIA 成功几率。现有的研究进展主要集中在以下方面：</p>\n<ol>\n<li>差分隐私与 MIA 的关系，这一方面已有相关的理论和证明，但在实际应用上的评估并未取得较好的效用</li>\n<li>隐私 - 效用权衡：现有的研究表明当前的差分隐私在这一方面性能不够好，相关的研究表明，少数群体更易受 MIAs 影响，且差分隐私降低了这些群体的模型效用。</li>\n<li>训练方法：现有的方法主要是 DP-SGD，现在也有 DP-Logits 等新方法被提出</li>\n<li>生成模型中的应用：现有的研究表明，差分隐私也可以用于防御生成模型中的 MIAs，其防御效果与生成质量与隐私预算𝜖相关。并有研究表明 DP 各异限制过拟合，减轻 MIA</li>\n</ol>\n<p>DP 为保护训练记录的成员隐私提供了理论保障，可以用于缓解分类模型和生成模型中的 MIAs，无论攻击者是黑盒还是白盒设置。尽管 DP 应用广泛且有效，但一个缺点是它在复杂学习任务中难以提供可接受的隐私效用权衡。此外，DP 还可以用于缓解其他形式的隐私攻击，如属性推断攻击和特性推断攻击，并与对抗样本的模型鲁棒性有关。</p>\n<h1 id=\"可能的方向以及应用\"><a class=\"markdownIt-Anchor\" href=\"#可能的方向以及应用\">#</a> 可能的方向以及应用</h1>\n<h2 id=\"针对攻击方向\"><a class=\"markdownIt-Anchor\" href=\"#针对攻击方向\">#</a> 针对攻击方向</h2>\n<ol>\n<li><strong>针对正则化模型的攻击</strong>：MIA 系统通常依赖于机器学习系统的过拟合，而随着正则化技术的发展，这一假设收到挑战；目前针对过拟合模型的攻击仍处于未知状态。</li>\n<li><strong>针对自监督模型的攻击</strong>：目前，自监督模型开始广泛用于自然语言处理以及计算机视觉方面，对此类模型的攻击仍处于未知状态。</li>\n<li><strong>针对对抗性机器学习的攻击</strong>：对抗机器学习与成员推断攻击具有一定的共同性和差异，如何将对抗机器学习和成员推断攻击结合起来是其中可能的研究方向<br>\n 4.<strong> 针对对比学习（Contrastive learning aims）和元学习（Meta-learning）等新型机器学习模型的攻击</strong>：此类模型与传统的机器学习有很多差异，针对此类尚有较多领域亟待研究<br>\n 5.<strong> 针对联邦学习的攻击</strong>：现有的 MIA 主要适用于同质化的联邦学习，对于异构化的联邦学习研究不多<br>\n 6.<strong>MIA 相关的应用</strong>：如联邦学习中的来源推断攻击以及更加深入的隐私保护研究，通过 MIA 审计数据记录对 ML 模型的训练贡献等应用。</li>\n</ol>\n<h2 id=\"针对防御方向\"><a class=\"markdownIt-Anchor\" href=\"#针对防御方向\">#</a> 针对防御方向</h2>\n<ol>\n<li><strong>针对非监督学习模型的防御</strong>：非监督学习模型由于缺乏数据标签，因而难以处理过拟合，在这一方面的研究受到限制</li>\n<li><strong>针对生成模型的防御</strong>：可能的方向包括采用知识蒸馏、增强学习等方法进行防御，通过生成模型输出用于训练以避免原始数据的泄露。</li>\n<li><strong>针对隐私与效用的平衡</strong>：现有的差分隐私保护通常会对分类器的梯度添加大量噪声，由此会降低其预测精度，如何达成隐私和效用的平衡仍待研究</li>\n<li><strong>针对联邦学习的隐私防御</strong>： 目前联邦学习面临着越来越多的隐私攻击，需要开发相应的防御技术，差分隐私等技术在联邦学习上的应用是未来可能的一些方向</li>\n</ol>\n<p>（未完，针对未来的方向目前有些想法，后面会单开一章简单介绍）</p>\n",
            "tags": []
        }
    ]
}