{
    "version": "https://jsonfeed.org/version/1",
    "title": "Âº†ÂâçÁöÑÂ∞èÂ±ã ‚Ä¢ All posts by \"ÈòÖËØªÁ¨îËÆ∞\" category",
    "description": "",
    "home_page_url": "https://ckti.github.io",
    "items": [
        {
            "id": "https://ckti.github.io/2024/10/31/MIA_2/",
            "url": "https://ckti.github.io/2024/10/31/MIA_2/",
            "title": "Membership Inference Attacks Against Recommender Systems",
            "date_published": "2024-10-31T05:54:17.000Z",
            "content_html": "<h1 id=\"background-and-problem\"><a class=\"markdownIt-Anchor\" href=\"#background-and-problem\">#</a> Background and Problem</h1>\n<h2 id=\"backgrounds-of-recommend-system\"><a class=\"markdownIt-Anchor\" href=\"#backgrounds-of-recommend-system\">#</a> Backgrounds of recommend system</h2>\n<p>The recommender system is essentially an information filtering system, relying on machine learning algorithms to predict user preferences for items. There are two main methods in these realm:</p>\n<p><strong>collaborative filtering</strong>: using traditional methods, combining the history action and similar actions to make decisions</p>\n<p><strong>content-based recommendation</strong>:  using meta-data to distinguish the user preferences.</p>\n<p>The success of recommender systems lies in the large- scale user data. However, the data in many cases contains sensitive information of individuals.</p>\n<h2 id=\"contribution-of-this-paper\"><a class=\"markdownIt-Anchor\" href=\"#contribution-of-this-paper\">#</a> Contribution of this paper</h2>\n<p>First quantification of privacy risks in recommender systems through user-level membership inference attacks.</p>\n<p>Overcoming technical challenges by addressing limited access to ranked recommendations without posterior probabilities.</p>\n<p>Introducing a shadow recommender system to generate labeled data for attack models.</p>\n<p>Extensive experiments demonstrating strong attack performance across benchmark datasets.</p>\n<p>Proposing a defense mechanism called ‚ÄúPopularity Randomization‚Äù to reduce attack success by adding randomness.</p>\n<h1 id=\"methods\"><a class=\"markdownIt-Anchor\" href=\"#methods\">#</a> Methods</h1>\n<h2 id=\"labeled-data-generation\"><a class=\"markdownIt-Anchor\" href=\"#labeled-data-generation\">#</a> Labeled Data Generation</h2>\n<p>The adversary builds a shadow recommender system to mimic the target recommender system and generate training data. This involves factorizing a user-item rating matrix to project users and items into a shared latent space. The adversary then calculates the center vectors of user interactions and recommendations, and the difference between these vectors forms the user feature vector. Users are labeled as members (1) or non-members (0).</p>\n<h2 id=\"attack-model-establishment\"><a class=\"markdownIt-Anchor\" href=\"#attack-model-establishment\">#</a> Attack Model Establishment</h2>\n<p>A multi-layer perceptron (MLP) with two hidden layers is used as the attack model to infer membership status. The MLP is trained on the feature vectors generated in the previous step, and outputs probabilities indicating membership.</p>\n<h2 id=\"parameter-optimization\"><a class=\"markdownIt-Anchor\" href=\"#parameter-optimization\">#</a> Parameter Optimization</h2>\n<p>The MLP is trained using stochastic gradient descent, minimizing a cross-entropy loss function. After training, the attack model uses the test data to infer the membership status of users in the target recommender system.</p>\n<h2 id=\"framework\"><a class=\"markdownIt-Anchor\" href=\"#framework\">#</a> Framework</h2>\n<p>This method leverages both user interactions and recommendation patterns, and captures the order information from the ranked lists of recommended items, a critical aspect that distinguishes it from previous membership inference attacks .</p>\n<p><img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/frameworkk_MIA4rec.png\" alt=\"The framework of the proposed method\"></p>\n<h1 id=\"experiments\"><a class=\"markdownIt-Anchor\" href=\"#experiments\">#</a> Experiments</h1>\n<h2 id=\"experimental-target\"><a class=\"markdownIt-Anchor\" href=\"#experimental-target\">#</a> Experimental target</h2>\n<p><strong>Target Models</strong>: The experiment use three Personalized recommendation algorithms  for members: Latent Factor Model (LFM) and Neural Collaborative Filtering (NCF) ; And use popularity recommendation algorithm for non-members due to the lack of non-members‚Äô data.</p>\n<p><strong>Data Sets</strong>: This paper use three real-world datasets that is widely used for experiments of recommend systems, including Amazon Digital Music (ADM) , Lastfm-2k (lf-2k) , and Movielens-1m (ml-1m) .</p>\n<h2 id=\"data-preprocessing\"><a class=\"markdownIt-Anchor\" href=\"#data-preprocessing\">#</a> Data Preprocessing</h2>\n<p>For each dataset, the paper divide it into three disjoint subsets: shadow dataset, a target dataset and a dataset for extracting item features. Then , the paper processes these to make the data suitable:</p>\n<ol>\n<li>To generate feature vectors for users, the dataset for item feature should contain all items of the target and shadow recommenders.</li>\n<li>For the shadow or target dataset, the paper further divide it into two disjoint parts, which are used to conduct recommendations to members and non-members, respectively.</li>\n<li>The paper filtered out the users who have less than 20 interactions.</li>\n</ol>\n<h2 id=\"evaluation-metrics\"><a class=\"markdownIt-Anchor\" href=\"#evaluation-metrics\">#</a> Evaluation Metrics</h2>\n<p>This paper use AUC (area under the ROC curve) as the metric to evaluate attack performances. Regarding members as positive data points and non-members as negative data points. AUC indicates the proportion of the prediction results being positive to negative.</p>\n<h2 id=\"implementation\"><a class=\"markdownIt-Anchor\" href=\"#implementation\">#</a> Implementation</h2>\n<p>The attack model is a multi-layer perceptron (MLP) with two hidden layers. The first layer has 32 units, and the second has 8 units. Stochastic Gradient Descent (SGD) is used as the optimizer, with a learning rate of 0.01 and a momentum of 0.7. The model is trained for 20 epochs.</p>\n<h2 id=\"data-preprocessing-2\"><a class=\"markdownIt-Anchor\" href=\"#data-preprocessing-2\">#</a> Data Preprocessing</h2>\n<p>For each dataset, the paper divide it into three disjoint subsets: shadow dataset, a target dataset and a dataset for extracting item features. Then , the paper processes to make the data suitable:<br>\nTo generate feature vectors for users, the dataset for item feature should contain all items of the target and shadow recommenders.<br>\nFor the shadow or target dataset, the paper further divide it into two disjoint parts, which are used to conduct recommendations to members and non-members, respectively.<br>\nThe paper filtered out the users who have less than 20 interactions.</p>\n<h2 id=\"recommendation-performance\"><a class=\"markdownIt-Anchor\" href=\"#recommendation-performance\">#</a> Recommendation Performance</h2>\n<p>This paper use HR@100 (HR means hit rate)to evaluate the recommendation performance. The result shows that the Recommendation systems works better on Ml-1m data sets. When using Item methods , the HR is up to about 0.95</p>\n<p><img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/MIA2_RecPerf.png\" alt=\"\"></p>\n<h2 id=\"attack-performance\"><a class=\"markdownIt-Anchor\" href=\"#attack-performance\">#</a> Attack Performance</h2>\n<p>In this part, the paper evaluated three assumptions and evaluate the performance of the attack model for each assumption:</p>\n<h3 id=\"assumption-i\"><a class=\"markdownIt-Anchor\" href=\"#assumption-i\">#</a> Assumption I</h3>\n<p>the attacker knows both the algorithm and the data distribution of the target recommender system. Under this assumption, the experimental results show very strong attack performance. As the figure in the paper,when the shadow recommender system mirrors the target recommender system‚Äôs algorithm and data, the AUC scores are highly accurate.</p>\n<p><img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/MIA2_Assumption1.png\" alt=\"The result for the attack performance under Assumption I\"></p>\n<h3 id=\"assumption-ii\"><a class=\"markdownIt-Anchor\" href=\"#assumption-ii\">#</a> Assumption II</h3>\n<p>The attacker only knows the data distribution used to train the target recommender system but does not know the specific recommendation algorithm. Under this assumption, the attack performance decreases but remains strong. For example, in the ADM dataset, when the target system uses the Item algorithm and the shadow system uses the LFM algorithm, the AUC drops from 0.926 to 0.843, showing the impact of data distribution similarity on the attack.</p>\n<p><img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/MIA2_Assumption2.png\" alt=\"The result for the attack performance under Assumption II\"></p>\n<h3 id=\"assumption-iii\"><a class=\"markdownIt-Anchor\" href=\"#assumption-iii\">#</a> Assumption III</h3>\n<p>The attacker only knows the data distribution used to train the target recommender system but does not know the specific recommendation algorithm. Under this assumption, the attack performance decreases but remains strong. For example, in the ADM dataset, when the target system uses the Item algorithm and the shadow system uses the LFM algorithm, the AUC drops from 0.926 to 0.843, showing the impact of data distribution similarity on the attack.</p>\n<p><img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/MIA2_Assumption3.png\" alt=\"The result for the attack performance under Assumption III\"></p>\n<h3 id=\"hyperparameters\"><a class=\"markdownIt-Anchor\" href=\"#hyperparameters\">#</a> HyperParameters</h3>\n<p>In this paper, the impact of hyperparameters on the success of the attack is reflected in the following aspects:</p>\n<ol>\n<li><strong>Number of recommendations (k)</strong>: With more recommended items, the attack model gains more information, but performance improvements cease when the number is large enough.</li>\n<li><strong>Length of feature vectors (l)</strong>: Longer feature vectors provide more dimensional information to the model, but after a certain point, further increasing the length no longer significantly boosts performance.</li>\n<li><strong>Weights of recommended items</strong>: When considering the order of items in the recommendation list, attack performance significantly improves. Items at the front of the list are more likely to be accepted by users, and assigning higher weights to these items enhances the effectiveness of the attack.</li>\n</ol>\n<h1 id=\"defence\"><a class=\"markdownIt-Anchor\" href=\"#defence\">#</a> Defence</h1>\n<h2 id=\"popularity-randomization\"><a class=\"markdownIt-Anchor\" href=\"#popularity-randomization\">#</a> Popularity Randomization</h2>\n<p>non-members are provided with the most popular items. As a result, feature vectors of non-members are extremely similar and easily distinguished from members.  To fix this problems, the paper bring forward the popularity randomization. By selecting candidates from the most popular items randomly to make the target vectors harder to be distinguished.</p>\n<h2 id=\"evaluation-results\"><a class=\"markdownIt-Anchor\" href=\"#evaluation-results\">#</a> Evaluation results</h2>\n<p>The figure of the test result shows that when using the popularity randomization, the AUC rate was apparently downed for most of the data sets, and some of them even close to 0.5, which is nearly close to random guessing.</p>\n<p><img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/MIA2_Defence.png\" alt=\"The result of popularity randomization\"></p>\n<h1 id=\"discussion-and-conclusion\"><a class=\"markdownIt-Anchor\" href=\"#discussion-and-conclusion\">#</a> Discussion and conclusion</h1>\n<h2 id=\"the-factors-for-effects-of-attacks\"><a class=\"markdownIt-Anchor\" href=\"#the-factors-for-effects-of-attacks\">#</a> The Factors for effects of attacks</h2>\n<ol>\n<li><strong>The Choice of Datasets</strong>: The dataset with a denser user-item matrix leads to better attack performances. Attack models works better in the datasets with rich informations.</li>\n<li><strong>The Selection of Recommendation Algorithms</strong>: The recommender system with simple model structure is easier to be attacked. Compare with more complex algorithm,LFM has higher model complexity, which makes it harder to attack.</li>\n<li><strong>Distributions of Generated User Features</strong>:  When the distribution of user feature vectors generated by the shadow recommender system closely matches the target system, attack performance improves. The similarity between target data and target data is crucial for boosting attack effectiveness.</li>\n</ol>\n<h2 id=\"conclusions\"><a class=\"markdownIt-Anchor\" href=\"#conclusions\">#</a> Conclusions</h2>\n<h2 id=\"effectiveness-of-the-attack\"><a class=\"markdownIt-Anchor\" href=\"#effectiveness-of-the-attack\">#</a> Effectiveness of the Attack:</h2>\n<p>The proposed membership inference attack model demonstrates strong performance across various recommender systems and datasets. Even with limited knowledge, the model can effectively infer user membership.</p>\n<h2 id=\"proposed-defense-mechanism\"><a class=\"markdownIt-Anchor\" href=\"#proposed-defense-mechanism\">#</a> Proposed Defense Mechanism</h2>\n<p>To mitigate the attack, the authors introduce the ‚ÄúPopularity Randomization‚Äù defense mechanism. Experiments show that this defense significantly reduces the success rate of the attack, especially in complex models like Neural Collaborative Filtering (NCF).</p>\n<h2 id=\"future-work\"><a class=\"markdownIt-Anchor\" href=\"#future-work\">#</a> Future Work</h2>\n<p>The paper suggests further exploration of more effective defense mechanisms against such attacks and expanding the application of similar attacks and defenses to other machine learning models.</p>\n",
            "tags": []
        },
        {
            "id": "https://ckti.github.io/2024/09/28/MIA_1/",
            "url": "https://ckti.github.io/2024/09/28/MIA_1/",
            "title": "Membership Inference Attacks Against Machine Learning Models",
            "date_published": "2024-09-28T13:52:49.000Z",
            "content_html": "<h1 id=\"introduction-and-background\"><a class=\"markdownIt-Anchor\" href=\"#introduction-and-background\">#</a> Introduction and Background</h1>\n<h2 id=\"introduction-of-the-privacy-problem-in-machine-learning\"><a class=\"markdownIt-Anchor\" href=\"#introduction-of-the-privacy-problem-in-machine-learning\">#</a> Introduction of the Privacy Problem in Machine Learning</h2>\n<p>Machine learning is the foundation of popular Internet services such as image and speech recognition and natural language translation and recommend system, with many companies offering machine learning services via APIs (e.g., Google and Amazon).</p>\n<p>However, these models often learn sensitive user data during training, leading to potential privacy risks. Specifically, machine learning models can inadvertently leak their training data, posing a threat to user privacy. This paper focuses on ‚Äúmembership inference attacks,‚Äù where an attacker can infer whether a specific data point was part of the model‚Äôs training set based on its outputs.</p>\n<h2 id=\"machine-learning-background\"><a class=\"markdownIt-Anchor\" href=\"#machine-learning-background\">#</a> Machine Learning Background</h2>\n<p>Machine learning is categorized into supervised and unsupervised learning: Supervised learning uses labeled data to train models to predict outputs from inputs.</p>\n<p>A common issue in machine learning is overfitting, where the model performs well on training data but poorly on unseen data. Overfitted models are more likely to memorize training data, leading to privacy leaks, as they retain too much information about the data they trained on.</p>\n<p>Well-regularized models should avoid overfitting, generalizing well to new data without revealing sensitive information from the training data.</p>\n<h2 id=\"privacy-in-machine-learning\"><a class=\"markdownIt-Anchor\" href=\"#privacy-in-machine-learning\">#</a> Privacy in Machine Learning</h2>\n<p>Machine learning models can unintentionally leak sensitive information about the data they were trained on.</p>\n<p>Two primary types of privacy risks are:</p>\n<p><strong>Population-level inference</strong>: Inferring general patterns about the population used to train the model, which can reveal sensitive characteristics.</p>\n<p><strong>Membership inference</strong>: Determining if a specific individual‚Äôs data was included in the training set.</p>\n<p>This paper focuses on membership inference attacks, as protecting the privacy of training set members is both practical and critical for users.</p>\n<p>The risk is higher for models trained on private or sensitive data, such as healthcare records.</p>\n<h2 id=\"problem-statements\"><a class=\"markdownIt-Anchor\" href=\"#problem-statements\">#</a> Problem Statements</h2>\n<p>The core problem studied in this paper is membership inference attacks.<br>\nIn a black-box setting, the attacker can query the model to get outputs but does not have access to the model‚Äôs structure or parameters.<br>\nThe attacker‚Äôs goal is to determine if a specific data point was part of the model‚Äôs training set based on the model‚Äôs output.<br>\nThe paper assumes that the attacker might have some background knowledge, such as understanding the input format or statistical distribution of the dataset.<br>\nThe attack relies on detecting subtle differences in how the model behaves with data it has seen before (training data) versus new data (non-training data).</p>\n<h1 id=\"methods\"><a class=\"markdownIt-Anchor\" href=\"#methods\">#</a> Methods</h1>\n<h2 id=\"main-steps\"><a class=\"markdownIt-Anchor\" href=\"#main-steps\">#</a> Main steps:</h2>\n<p>The attacker queries the target model with a data record and obtains the model‚Äôs prediction on that record. The prediction is a vector of probabilities, one per class, that the record belongs to a certain class. This prediction vector, along with the label of the target record, is passed to the attack model, which infers whether the record was in or out of the target model‚Äôs training dataset.</p>\n<p><img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/Main_step.png\" alt=\"Main steps of MIA\"></p>\n<h2 id=\"shadow-models\"><a class=\"markdownIt-Anchor\" href=\"#shadow-models\">#</a> Shadow Models</h2>\n<p>The attacker uses the input and output data from shadow models to train the attack model. Specifically, the prediction results from the shadow models (confidence vectors or other outputs) are used as training data to train a binary classifier that can predict whether a particular data point was part of the shadow model‚Äôs training set.</p>\n<p>Since the behavior of the shadow model is similar to that of the target model, the attack model can learn from the shadow model‚Äôs training to infer which data points were used in the training of the target model.</p>\n<p><img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/Shadow_Models.png\" alt=\"Shadow Models\"></p>\n<h2 id=\"training-the-attack-model\"><a class=\"markdownIt-Anchor\" href=\"#training-the-attack-model\">#</a> Training the Attack Model</h2>\n<p>The inputs and outputs of the shadow models are used to train the attack modelÔºö</p>\n<p>The attack model is a binary classifier that learns to distinguish between ‚Äútraining data‚Äù (members) and‚Äúnon-training data‚Äù (non-members).</p>\n<p>It uses the prediction vectors from the shadow models to learn how to classify data points as members or non-members.</p>\n<p><img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/Training_attack_models.png\" alt=\"Main steps of training the attack models\"></p>\n<h1 id=\"experimental-evaluation\"><a class=\"markdownIt-Anchor\" href=\"#experimental-evaluation\">#</a> Experimental Evaluation</h1>\n<h2 id=\"datasets-and-target-models\"><a class=\"markdownIt-Anchor\" href=\"#datasets-and-target-models\">#</a> Datasets and Target Models</h2>\n<p>The datasets used for experiments are described, including the type, size, and nature of the data, includes: Public datasets such as image datasets (e.g., CIFAR-10), location datasets, and some sensitive data sets .</p>\n<p>In this paper, the authorevaluated our inference attacks on three types of target models: two constructed by cloud-based ‚Äúmachine learning as a service‚Äù platforms and one implemented locally. all the attacks treat the models as black boxes, which means they do not know the type or structure of the models they create, nor the values of the hyper-parameters used during the training process.</p>\n<p>The training set and the test set of each target and shadow model are randomly selected from the respective datasets, have the same size, and are disjoint. There is no overlap between the datasets of the target model and those of the shadow models, but the datasets used for different shadow models can overlap with each other.</p>\n<h2 id=\"accuracy-of-the-attack\"><a class=\"markdownIt-Anchor\" href=\"#accuracy-of-the-attack\">#</a> Accuracy of the attack</h2>\n<p>This paper evaluate the attack by executing it on randomly reshuffled records from the target‚Äôs training and test datasets. And use the standard precision and recall metric to evaluate the percision.</p>\n<p>The test accuracy of the target neural-network models with the largest training datasets is low, which means the models are heavily overfitted on their training sets.</p>\n<p>For different deep learning APIs from different companies, the paper trained the same datas for the models and make the evaluation attacks, the result shows that Models trained using Google Prediction API exhibit the biggest leakage.</p>\n<p>For the Texas hospital-stay dataset and location adtaset, the paper evaluated the attack against a Google-trained model.</p>\n<p>The training accuracy of the Texas‚Äôs target model is 0.66 and its test accuracy is 0.51. Precision is mostly above 0.6, and for half of the classes, it is above 0.7. Precision is above 0.85 for more than 20 classes.</p>\n<p>The training accuracy of the location‚Äôs target model is 1 and its test accuracy is 0.66.Precision is between 0.6 and 0.8, with an almost constant recall of 1.<br>\nThe attacks against the google trained models for location data sets shows that the paper‚Äôs attacks are robust even if the attacker‚Äôs assumptions about the distribution of the target model‚Äôs training data are not very accurate.</p>\n<p>For the majority of the target model‚Äôs classes, the paper‚Äôs attack achieves high precision. This demonstrates that a membership inference attack can be trained with only black-box access to the target model, without any prior knowledge about the distribution of the target model‚Äôs training data if the attacker can efficiently generate inputs that are classified by the target model with high confidence.</p>\n<h1 id=\"factors-and-defenses\"><a class=\"markdownIt-Anchor\" href=\"#factors-and-defenses\">#</a> Factors and Defenses</h1>\n<h2 id=\"factors-for-success-of-membership-inference\"><a class=\"markdownIt-Anchor\" href=\"#factors-for-success-of-membership-inference\">#</a> Factors for Success of membership inference</h2>\n<p>Based on the evaluation, this paper bring forward two factors for a success membership inference attack:</p>\n<p><strong>generalizability of the target model</strong></p>\n<p><strong>diversity of its training data</strong></p>\n<p>If the model overfits and does not generalize well to inputs beyond its training data, or if the training data is not representative, the model leaks information about its training inputs.</p>\n<p>This paper also point out that overfitting is not the only reason why our inference attacks work. Different machine learning models, due to their different structures, ‚Äúremember‚Äù different amounts of information about their training datasets. This leads to different amounts of information leakage even if the models are overfitted to the same degree</p>\n<h2 id=\"mitigation-strategies\"><a class=\"markdownIt-Anchor\" href=\"#mitigation-strategies\">#</a> Mitigation strategies</h2>\n<p>This paper indicates some strategies of mintigate the membership interface of the models:</p>\n<ol>\n<li>Restrict the prediction vector to top k classes.</li>\n<li>CoarsenÔºàÂèòÁ≤óÁ≥ôÔºâ precision of the prediction vector.</li>\n<li>Increase entropy of the prediction vector.</li>\n<li>Use regularization.</li>\n</ol>\n<p>However, in this paper, they make evaluation about these mitigation strategies, and find that their attack method is still robust against these mitigation strategies</p>\n<h1 id=\"conclusion\"><a class=\"markdownIt-Anchor\" href=\"#conclusion\">#</a> Conclusion</h1>\n<p>This paper have designed, implemented, and evaluated the first membership inference attack against machine learning models, notably black-box models trained in the commerical deep learning APIs.</p>\n<p>This paper‚Äôs key technical innovation is the shadow training tech- nique that trains an attack model to distinguish the target model‚Äôs outputs on members versus non-members of its train- ing dataset.</p>\n<p>Membership in hospital-stay and other health-care datasets is sensitive from the privacy perspective. Therefore, this method may have substantial practical privacy implications.</p>\n",
            "tags": []
        },
        {
            "id": "https://ckti.github.io/2024/09/28/Introduction%20of%20MIA/",
            "url": "https://ckti.github.io/2024/09/28/Introduction%20of%20MIA/",
            "title": "Introduction of Membership Interface Attack",
            "date_published": "2024-09-28T13:52:47.000Z",
            "content_html": "<p>This is the English version of <a href=\"/public/2024/08/02/Introduction%20of%20MIA_CN/index.html\">this</a> blog. The translation may be not correct, if there are any mistakes, please contact me for delivering suggestions.</p>\n<h1 id=\"prelimaries-defination-and-classification\"><a class=\"markdownIt-Anchor\" href=\"#prelimaries-defination-and-classification\">#</a> Prelimaries ,Defination and classification</h1>\n<h2 id=\"prelimaries-machine-learning\"><a class=\"markdownIt-Anchor\" href=\"#prelimaries-machine-learning\">#</a> Prelimaries-- Machine learning</h2>\n<p>Machine learning (ML) is the study of computer algorithms that improve automatically through experience and by learning from data. Generally, it can be  be divided into these two categories</p>\n<p><strong>Supervised Learning</strong> :Uses labeled data where input-output pairs are known to train models. It aims to predict or classify new data based on learned patterns, minimizing the error between predicted and actual outcomes.</p>\n<p><strong>Unsupervised Learning</strong> :Works with unlabeled data to identify hidden patterns or structures. It clusters or finds relationships within the data without predefined labels.</p>\n<h2 id=\"membership-interface-attackdefination\"><a class=\"markdownIt-Anchor\" href=\"#membership-interface-attackdefination\">#</a> Membership Interface Attack‚ÄìDefination</h2>\n<p>Membership interface attack (MIA) is a method of obtaining the data being trained in the targeted ML models byThe main thought of MIA is forming a models that can repersents the membership of the target models. training similar model called shadow model and training a classifier to check whether a data is being trained in the targeted ML model.</p>\n<h3 id=\"shadow-models\"><a class=\"markdownIt-Anchor\" href=\"#shadow-models\">#</a> Shadow models</h3>\n<p>The main thought of MIA is forming a models that can repersents the membership of the target models.</p>\n<p><img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/Shadow%20model.png\" alt=\"shadow models and training\"></p>\n<h2 id=\"basic-taxonomy-of-mia\"><a class=\"markdownIt-Anchor\" href=\"#basic-taxonomy-of-mia\">#</a> Basic Taxonomy of MIA</h2>\n<h3 id=\"based-on-informations\"><a class=\"markdownIt-Anchor\" href=\"#based-on-informations\">#</a> based on informations</h3>\n<p>Based on the informations of the target model attackers can get, the MIA can be divided into these two categories:</p>\n<p><strong>White-box attack</strong>: Attackers can obtain all the information about the ML model, including the data distributions, training methods and relevant  parameters</p>\n<p><strong>Black-box attack</strong>: Attackers can only get restricted informations, including restricted data distributions, training methods and parameters.</p>\n<p>Compared with white-box attack, black-box attacks gets fewer informations, which leads it harder to achieve. However, the influence of a successful attacks can be  larger than it. Today , the mainstream of the research is also focusing on the black-box attacks.</p>\n<h3 id=\"based-on-prediction-vectors\"><a class=\"markdownIt-Anchor\" href=\"#based-on-prediction-vectors\">#</a> based on Prediction vectors</h3>\n<p>The prediction vectors is a parameter for judging whether a data is in the models. Today, the main research of MIA can also be categorized based on prediction vectors in the following graph:</p>\n<p><img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/Prediction%20vectors.png\" alt=\"Taxonomy of Prediction vectors\"></p>\n<h3 id=\"based-on-the-judgement-methods\"><a class=\"markdownIt-Anchor\" href=\"#based-on-the-judgement-methods\">#</a> Based on the judgement methods</h3>\n<h4 id=\"binary-classifier-based-mia\"><a class=\"markdownIt-Anchor\" href=\"#binary-classifier-based-mia\">#</a> Binary Classifier Based MIA.</h4>\n<p>The data can be classified as members or non-members by training a binary classifier for classify the models. the main method is as followed:</p>\n<p><strong>Training the shadow models</strong>: Attackers use multiple shadow models that have the same or similar distributions of the target training sets to learn and training the shadow models.</p>\n<p><strong>Collecting prediction models</strong>: Attackers will make search for the shadow training sets and get the prediction vectors of each data records. Each data of the vectors in shadow training data sets can be labeled as ‚ÄúMember‚Äù and the data in test data sets are labled ‚ÄúNon-member‚Äù</p>\n<p><strong>Training the attack models</strong>: Form the ‚Äúmember‚Äùand ‚Äúnon-member‚Äùdata sets based on the labeled datas , and training a binary classifier to form it.</p>\n<p>By this, Identifying the complex problem of recognizing the member and non-member of the model is converted into the binary classifier problem.</p>\n<h4 id=\"metric-based-membership-inference-attacks\"><a class=\"markdownIt-Anchor\" href=\"#metric-based-membership-inference-attacks\">#</a> Metric Based Membership Inference Attacks</h4>\n<p>Metric Based MIA obtain the relative metrics by collecting and analysing the prediction vectors and make analyzations by comparing the metrics and thresold values.</p>\n<p>Compare with training the binary classifier, it is more simple and consume less computational resources. The next page will show the recent research realms of the analyse and settings.</p>\n<p>Currently, the research on metric based MIA have these categories:</p>\n<p><strong>Prediction Correctness Based MIA</strong>: If the target models predict the input data x correctly, then attackers recognized it as a member. The inituation of it is that if a data is in the real data, then the target model will predict the input data x correctly.</p>\n<p><strong>Loss rate Based MIA</strong>: If the difference of the loss rate correspond to the target models and the lossrate of the origin data is less than a thresold, then attackers recognize it as a member. The inituation of it is that if the input data is in the true datas, then the loss rate of the target models is near to the total loss rate.</p>\n<p><strong>Prediction Confidence Based MIA</strong>: If the prediction confidence of some records is larger than some thresold, then recognize it as a member. The inituation of it is that the target models will minimize the difference between it and the real models, so the prediction confidence will close to 1.</p>\n<p><strong>Prediction Entropy Based MIA</strong>: If the prediction entropy of input a records is lower than a thresold, then recognize it as a member. The inituation is that the target model‚Äôs prediction entropy of the training data is larger than the prediction of entropy of the test data.</p>\n<p><strong>Modified prediction Entropy Based MIA</strong>: Some opinion suggest that current prediction loss didn‚Äôt think of the ground truth label, so it may make some misjudgement of some datas, so in some papers, the algorithm of the prediction entropy is modified.</p>\n<h1 id=\"relavant-research\"><a class=\"markdownIt-Anchor\" href=\"#relavant-research\">#</a> Relavant research</h1>\n<h2 id=\"research-on-the-classification-models\"><a class=\"markdownIt-Anchor\" href=\"#research-on-the-classification-models\">#</a> Research on the classification models</h2>\n<p>Since Shokri et al. introduced this attack method, there has been a growing body of research focused on this direction. Salem et al. discussed the assumptions of Membership Inference Attacks (MIA) and attempted to relax the implementation conditions, demonstrating that two of the shadow model assumptions are not necessary and proposing an indicator-based MIA approach. Yeom et al. also proposed two indicator-based MIA methods; Long et al. achieved MIA attacks on certain data by focusing on data with unique effects on the target model, enabling accurate inference in generalized models with similar training and testing accuracy.</p>\n<p>Additionally, existing research has also targeted more restricted MIAs. Li and Zhang proposed transfer-based and perturbation-based MIAs. Transfer-based MIAs construct shadow models to simulate the target model, using the shadow model‚Äôs confidence to determine membership; perturbation-based MIAs introduce noise to create adversarial examples and distinguish members based on the severity of the perturbation. Choquette et al. introduced data augmentation-based MIAs and decision boundary distance-based MIAs. Data augmentation attacks target common data augmentation phenomena in machine learning systems, creating additional records through different augmentation strategies to query the target model for predictions. Decision boundary attacks estimate the distance of records to the model boundary, similar to Li and Zhang‚Äôs attacks. Successful MIA cases suggest that machine learning models may be more vulnerable to MIA than previously anticipated.</p>\n<p>Aside from black-box MIA attacks, Nasir et al. introduced white-box MIA, which can be seen as an extension of black-box MIA, enhancing attack efficiency by leveraging additional information. They use the gradient of the target model‚Äôs prediction loss for inference and train a model to distinguish between members and non-members using the SGD algorithm. However, Leino and Fredrikson pointed out that the assumptions of this method are too stringent, requiring attackers to know the approximate distribution of the target dataset. They proposed a Bayes-optimal attack-based MIA method, enabling MIA without background knowledge of the target model.</p>\n<h2 id=\"research-on-generative-models\"><a class=\"markdownIt-Anchor\" href=\"#research-on-generative-models\">#</a> Research on Generative Models</h2>\n<p>Corrent research on generative models are focusing on the generative adversarial network, whose models can be shown as follows:</p>\n<p><img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/GAN_architecture.png\" alt=\"\"></p>\n<p>Hayes et al. were the first to propose Membership Inference Attacks (MIA) against generative models. For white-box attacks, attackers collect all records and compute confidence scores to make inferences; for black-box attacks, attackers collect records from the generator to train a local GAN that mimics the target GAN, and then use the local GAN discriminator for inference. Hilprecht et al. introduced two additional attacks: a Monte Carlo-based black-box attack and a VAE-based white-box attack. Hilprecht et al. proposed a set-based attack to determine whether a data point belongs to a set, while Liu et al. introduced a similar co-membership inference attack, determining dataset membership by analyzing the distance of a data point to the target data. Chen et al. proposed a general method where attackers continuously reconstruct the attack model through optimization, calculate the distance between the generated results of the attack model and the target model, and estimate the probability of data membership based on this distance.</p>\n<h2 id=\"research-on-embedding-models\"><a class=\"markdownIt-Anchor\" href=\"#research-on-embedding-models\">#</a> Research on Embedding ModelsÔºö</h2>\n<p>Current research primarily focuses on text and image embedding models. For text embedding models, attacks aim to infer membership of words or sentence pairs within a sliding window, using similarity scores to determine if they belong to a predefined set. For graph embedding models, attack methods involve using shadow models and confidence scores to infer whether nodes in the graph belong to specific categories, addressing node classification issues.</p>\n<h2 id=\"research-on-regression-models\"><a class=\"markdownIt-Anchor\" href=\"#research-on-regression-models\">#</a> Research on Regression ModelsÔºö</h2>\n<p>Gupta et al. were the first to conduct MIA (Membership Inference Attack) research on regression models for age prediction, achieving attacks through the construction of a white-box binary classification model.</p>\n<h2 id=\"research-in-federated-learning\"><a class=\"markdownIt-Anchor\" href=\"#research-in-federated-learning\">#</a> Research in Federated LearningÔºö</h2>\n<p>In federated learning, attackers can be either the central server or some of the participating clients. They can implement MIA by determining whether certain data was used in training the global model. Melis was the first to propose a gradient-based MIA by analyzing the update mechanism of the RNN training embeddings. Turex introduced heterogeneous FL (Federated Learning), which involves analyzing differences in aggregated parameters from different clients. Nasr et al. discussed how gradient ascent attacks can actively interfere with FL training. Hu et al. proposed source inference attacks aimed at determining which participants hold training records in FL. They argue that existing MIA attacks in FL overlook the source information of training members, which could lead to further privacy issues.</p>\n<h1 id=\"factors-for-a-success-mia\"><a class=\"markdownIt-Anchor\" href=\"#factors-for-a-success-mia\">#</a> Factors for a success MIA</h1>\n<h2 id=\"overfitting-of-target-models\"><a class=\"markdownIt-Anchor\" href=\"#overfitting-of-target-models\">#</a> Overfitting of Target Models</h2>\n<p>Many studies have pointed out that overfitting of target ML models is a significant factor in the leakage of original datasets. Specifically:Models like DNNs, due to their high parameterization in applications, enhance their ability to handle large datasets but also record a lot of irrelevant information.Training machine learning models often requires many epochs, making them more prone to memorizing the content of the dataset.Machine learning datasets cannot fully represent real-world data.Existing articles indicate that for a classification system overfitting on training data, attackers can achieve an attack success probability higher than 50% based on the correctness of randomly guessed predictions.</p>\n<h2 id=\"features-of-the-model-itself\"><a class=\"markdownIt-Anchor\" href=\"#features-of-the-model-itself\">#</a> Features of the Model ItselfÔºö</h2>\n<p>When the decision boundary of the target model is not sensitive to the training data used, the effectiveness of MIA attacks is low. Current research shows that among DNN models, logistic regression models, Naive Bayes models, k-nearest neighbor models, and decision tree models, decision tree models have the highest attack accuracy, while the simple Naive Bayes algorithm has the lowest.</p>\n<h2 id=\"diversity-of-the-training-dataset\"><a class=\"markdownIt-Anchor\" href=\"#diversity-of-the-training-dataset\">#</a> Diversity of the Training Dataset:</h2>\n<p>When the training dataset used by the target model is highly diverse, it helps the model generalize better to test data. Consequently, the impact of MIA on the model will be smaller.</p>\n<h2 id=\"attackers-knowledge-of-the-target-model\"><a class=\"markdownIt-Anchor\" href=\"#attackers-knowledge-of-the-target-model\">#</a> Attacker‚Äôs Knowledge of the Target Model:</h2>\n<p>Existing research on MIA generally makes certain assumptions about the attacker: the attacker knows the relevant distribution of the training data and can construct a suitable shadow dataset based on this distribution. High-accuracy shadow models constructed under this assumption are needed for effective attacks.</p>\n<h1 id=\"research-on-defense-against-mia\"><a class=\"markdownIt-Anchor\" href=\"#research-on-defense-against-mia\">#</a> Research on Defense against MIA</h1>\n<h2 id=\"confidence-score-masking\"><a class=\"markdownIt-Anchor\" href=\"#confidence-score-masking\">#</a> Confidence Score Masking</h2>\n<p>This method is mainly used for defending against black-box attacks by returning obfuscated true confidence scores to the classifier. It includes the following three approaches:</p>\n<ol>\n<li>The target classifier does not provide the full prediction vector but only the top few confidence scores.</li>\n<li>The target classifier only provides predicted labels when the attacker provides data input.</li>\n<li>Noise is added to the returned vector.</li>\n</ol>\n<p>These three methods affect the prediction vector but do not result in a loss of prediction accuracy.</p>\n<h2 id=\"regularization\"><a class=\"markdownIt-Anchor\" href=\"#regularization\">#</a> Regularization</h2>\n<p>Regularization mitigates MIA attack strength by reducing model overfitting.</p>\n<p>Existing regularization methods include traditional techniques such as L2-norm regularization, dropout, data augmentation, model stacking, early stopping, and label smoothing. These methods lower overfitting by reducing the impact of different test datasets on samples, thereby also reducing the intensity of MIA attacks.</p>\n<p>Additionally, specially designed regularization systems like adversarial regularization and Mixup + MMD (Maximum Mean Discrepancy) can also defend against MIA by introducing new regularization mechanisms to decrease the differences between members and non-members. Compared to masking techniques, regularization can resist both black-box and white-box attacks and can alter output parameters when modifying the output model.</p>\n<h2 id=\"knowledge-distallation\"><a class=\"markdownIt-Anchor\" href=\"#knowledge-distallation\">#</a> Knowledge Distallation</h2>\n<p>Knowledge distillation refers to the process of training a smaller student model using a larger teacher model to transfer knowledge from the large model to the small one, allowing the smaller model to achieve a similar level of approximation. Based on this, existing research has introduced methods such as DMP, CKD, and PCKD, they are generally called DMP (Distillation For Membership Privacy), whose steps are as followed:</p>\n<ol>\n<li>Train an unprotected teacher model to record and label data in an unlabeled dataset.</li>\n<li>Select data with lower prediction entropy for training, which is used for classification.</li>\n<li>Train based on the labeled model.</li>\n</ol>\n<p>Additionally, There are also some research that proposes Complementary Knowledge Distillation (CKD) and Pseudo Complementary Knowledge Distillation (PCKD) methods. In these methods, the transfer data for knowledge distillation comes from a private training set. CKD and PCKD eliminate the need for public data, which may be difficult to obtain in some applications, making knowledge distillation a more practical defense method for mitigating MIA attacks on machine learning models.</p>\n<h2 id=\"differencial-privacy\"><a class=\"markdownIt-Anchor\" href=\"#differencial-privacy\">#</a> Differencial Privacy</h2>\n<p>Differential privacy refers to protecting the original data by adding relevant noise to the dataset. When a deep learning model is trained with a model that incorporates differential privacy, if the privacy budget is small enough, the trained model will not retain specific user information. Therefore, different privacy models can limit the success rate of MIA attacks based solely on the model. Current research realms are shown in the next page:</p>\n<p>Differential privacy provides theoretical protection for member privacy in training records and can mitigate MIAs in classification and generative models, regardless of whether the attacker is in a black-box or white-box setting. Despite its widespread and effective application, a drawback is its difficulty in providing an acceptable privacy-utility tradeoff in complex learning tasks. Additionally, differential privacy can also mitigate other forms of privacy attacks, such as attribute inference attacks and feature inference attacks, and is related to the robustness of models against adversarial examples.</p>\n<p>Currently , the research realms about differencial privacy is:</p>\n<p><strong>The relationship between differential privacy and MIA</strong>: There are theoretical results and proofs on this, but practical evaluations have not achieved good utility.</p>\n<p><strong>Privacy-Utility Tradeoff</strong>: Existing research shows that current differential privacy performance in this regard is insufficient. Studies indicate that minority groups are more affected by MIAs, and differential privacy reduces model utility for these groups.</p>\n<p><strong>Training Methods</strong>: Current methods primarily include DP-SGD, with new methods like DP-Logits also being proposed.</p>\n<p><strong>Applications in Generative Models</strong>: Research shows that differential privacy can also defend against MIAs in generative models, with defense effectiveness related to generative quality and privacy budget ùúñ. Studies indicate that differential privacy limits overfitting and mitigates MIA.</p>\n<h1 id=\"possible-future-directions\"><a class=\"markdownIt-Anchor\" href=\"#possible-future-directions\">#</a> Possible future directions</h1>\n<h2 id=\"in-membership-inference-attacks\"><a class=\"markdownIt-Anchor\" href=\"#in-membership-inference-attacks\">#</a> In Membership Inference Attacks</h2>\n<p><strong>Attacks on Regularized Models</strong>: MIA systems often rely on the overfitting of machine learning systems, but this assumption is challenged by advancements in regularization techniques; attacks on overfitted models are still largely unexplored.</p>\n<p><strong>Attacks on Self-Supervised Models</strong>: Self-supervised models are becoming widespread in NLP and computer vision, and attacks on these models are still largely unknown.</p>\n<p><strong>Attacks on Adversarial Machine Learning</strong>: Adversarial machine learning shares some similarities and differences with membership inference attacks; combining these approaches could be a potential research direction.</p>\n<p><strong>Attacks on New Machine Learning Models</strong> (e.g., Contrastive Learning and Meta-Learning): These models differ significantly from traditional ones, and many areas for research remain in attacking them.</p>\n<p><strong>Attacks on Federated Learning</strong>: Existing MIAs are mainly applicable to homogeneous federated learning, with limited research on heterogeneous federated learning.</p>\n<p><strong>Applications Related to MIA</strong>: Includes source inference attacks in federated learning and deeper privacy protection studies through MIA audits of data contributions to ML models.</p>\n<h2 id=\"in-the-defence-of-membership-inference-attacks\"><a class=\"markdownIt-Anchor\" href=\"#in-the-defence-of-membership-inference-attacks\">#</a> In the Defence of Membership Inference Attacks</h2>\n<p><strong>Defense Against Unsupervised Learning Models</strong>: Unsupervised learning models struggle with overfitting due to a lack of data labels, and research in this area is limited.</p>\n<p><strong>Defense Against Generative Models</strong>: Possible defenses include methods such as knowledge distillation and reinforcemnt learning to avoid leakage of raw data through the outputs of generative models.</p>\n<p><strong>Balancing Privacy and Utility</strong>: Existing differential privacy protections often add significant noise to classifier gradients, reducing prediction accuracy. Balancing privacy and utility remains an area for research.</p>\n<p><strong>Privacy Defenses in Federated Learning</strong>: With increasing privacy attacks in federated learning, developing defensive technologies is crucial, with differential privacy being a potential future direction.</p>\n",
            "tags": []
        },
        {
            "id": "https://ckti.github.io/2024/09/28/Introduction%20of%20MIA_CN/",
            "url": "https://ckti.github.io/2024/09/28/Introduction%20of%20MIA_CN/",
            "title": "ÊàêÂëòÊé®ÁêÜÊîªÂáª‰ªãÁªç",
            "date_published": "2024-09-28T13:52:41.000Z",
            "content_html": "<p><a href=\"/https://arxiv.org/abs/2103.07853\">ËÆ∫ÊñáÂú∞ÂùÄ</a><br>\nÊ†áÈ¢ò: Membership Inference Attacks on Machine Learning: A Survey</p>\n<h1 id=\"mia-ÁöÑÂâçÁΩÆÁü•ËØÜÂíåÂÆö‰πâ\"><a class=\"markdownIt-Anchor\" href=\"#mia-ÁöÑÂâçÁΩÆÁü•ËØÜÂíåÂÆö‰πâ\">#</a> MIA ÁöÑÂâçÁΩÆÁü•ËØÜÂíåÂÆö‰πâ</h1>\n<h2 id=\"ÂÆö‰πâ\"><a class=\"markdownIt-Anchor\" href=\"#ÂÆö‰πâ\">#</a> ÂÆö‰πâ</h2>\n<p>ÊàêÂëòÊé®ÁêÜÊîªÂáª (Membership Inference Attacks) ÊòØÊåáÂØπ‰∏Ä‰∏™‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÔºåÈÄöËøáÁåúÊµãÂÖ∂‰∏≠ÁöÑÊï∞ÊçÆÂπ∂ËÆ≠ÁªÉ‰∏Ä‰∏™ shadow model ÁåúÊµãÂØπÂ∫îÊï∞ÊçÆÊòØÂê¶Âú®ÂÖ∂‰∏≠„ÄÇ</p>\n<h2 id=\"Âü∫Êú¨ÂàÜÁ±ª\"><a class=\"markdownIt-Anchor\" href=\"#Âü∫Êú¨ÂàÜÁ±ª\">#</a> Âü∫Êú¨ÂàÜÁ±ª</h2>\n<p>ÈíàÂØπ MIA ÁöÑÁõÆÊ†áÊ®°ÂûãÊÄßË¥®ÔºåÂèØÂ∞Ü MIA ÊîªÂáªÂàÜ‰∏∫‰ª•‰∏ã‰∏§ÁßçÔºåÂÖ∑‰ΩìÂèØ‰ª•ÂèÇËßÅ‰∏ãÂõæ:<br>\n<img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/B%26W_attacks.png\" alt=\"‰∏§ÁßçÊîªÂáªÁöÑÂ∑ÆÂºÇ\"></p>\n<p><strong>ÁôΩÁõíÊîªÂáª</strong>ÔºöÊîªÂáªËÄÖÂèØ‰ª•Ëé∑ÂèñÊ®°ÂûãÁöÑÊâÄÊúâ‰ø°ÊÅØÔºåÂåÖÊã¨Êï∞ÊçÆÂàÜÂ∏ÉÔºåËÆ≠ÁªÉÊñπÂºè‰ª•ÂèäÁõ∏ÂÖ≥ÂèòÈáè<br>\n<strong>ÈªëÁõíÊîªÂáª</strong>ÔºöÊîªÂáªËÄÖÂè™ËÉΩËøõË°åÈªëÁõíËÆøÈóÆÔºåËé∑ÂèñÂèóÈôêÁöÑÊï∞ÊçÆÂàÜÂ∏ÉÔºåËÆ≠ÁªÉÊñπÂºèÁ≠â‰∏≠Èó¥‰ø°ÊÅØ</p>\n<p>Áõ∏ËæÉ‰∫éÁôΩÁõíÊîªÂáªÔºåÈªëÁõíÊîªÂáªÁî±‰∫éÂèØËé∑ÂèñÁöÑ‰ø°ÊÅØËæÉÂ∞ëÔºåÂõ†Ê≠§ÂÆûÁé∞ÈöæÂ∫¶ËæÉÂ§ßÔºå‰ΩÜÊîªÂáªÊàêÂäüÈÄ†ÊàêÁöÑÂΩ±Âìç‰πüÊõ¥Â§ßÔºåÁé∞‰ªä‰∏ªË¶ÅÁöÑÁ†îÁ©∂ÊñπÂêë‰πüÈõÜ‰∏≠‰∫éÈªëÁõíÊîªÂáªÔºåÊ†πÊçÆÊèê‰æõÁöÑÈ¢ÑÊµãÂêëÈáèÔºåÂèØ‰ª•ÂàÜ‰∏∫ÔºöFull confidence scores „ÄÅTop-K confidence scores„ÄÅPrediction label only ‰∏âÁßçÊñπÂºèÔºåÂÖ∑‰Ωì‰ªãÁªçËßÅ‰∏ãÂõæÔºö<br>\n<img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/Prediction%20vectors.png\" alt=\"È¢ÑÊµãÂêëÈáè\"></p>\n<h1 id=\"miaÁöÑÊîªÂáªÁâπÁÇπÂíåÁõ∏ÂÖ≥Á†îÁ©∂\"><a class=\"markdownIt-Anchor\" href=\"#miaÁöÑÊîªÂáªÁâπÁÇπÂíåÁõ∏ÂÖ≥Á†îÁ©∂\">#</a> MIA ÁöÑÊîªÂáªÁâπÁÇπÂíåÁõ∏ÂÖ≥Á†îÁ©∂</h1>\n<h2 id=\"ÊîªÂáªÊ®°ÂûãÁöÑÁâπÁÇπ\"><a class=\"markdownIt-Anchor\" href=\"#ÊîªÂáªÊ®°ÂûãÁöÑÁâπÁÇπ\">#</a> ÊîªÂáªÊ®°ÂûãÁöÑÁâπÁÇπ</h2>\n<p>Êú∫Âô®Â≠¶‰π†Ê®°ÂûãÈÄöÂ∏∏‰ºöÈÄ†ÊàêËøáÊãüÂêàÔºåËøôÁßçËøáÊãüÂêà‰∏∫ MIA Êèê‰æõ‰∫Ü‰æøÂà©ÔºåÊ†πÊçÆÊâÄ‰ΩøÁî®ÁöÑÊîªÂáªÊ®°ÂûãÔºåÂèØ‰ª•Â∞Ü MIA ÂàÜ‰∏∫‰ª•‰∏ã‰∏§Á±ªÔºö</p>\n<h3 id=\"Âü∫‰∫é‰∫åÂÖÉÂàÜÁ±ªÂô®ÁöÑmia-binary-classifier-based-membership-inference-attacks\"><a class=\"markdownIt-Anchor\" href=\"#Âü∫‰∫é‰∫åÂÖÉÂàÜÁ±ªÂô®ÁöÑmia-binary-classifier-based-membership-inference-attacks\">#</a> Âü∫‰∫é‰∫åÂÖÉÂàÜÁ±ªÂô®ÁöÑ MIA ÔºàBinary Classifier Based Membership Inference Attacks.Ôºâ</h3>\n<p>ÈÄöËøáËÆ≠ÁªÉ‰∫åÂÖÉÂàÜÁ±ªÂô®ÁöÑ MIA ÂèØ‰ª•Â∞ÜÁõÆÊ†áÊ®°ÂûãÁöÑËÆ≠ÁªÉÊ®°Âûã‰∏éÈùûÊàêÂëòÂå∫ÂàÜÂºÄÊù•ÔºåÂú®Áé∞ÊúâÁöÑÁ†îÁ©∂‰∏≠ÔºåShokri Á≠â‰∫∫ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁß∞‰∏∫ÂΩ±Â≠êËÆ≠ÁªÉ (<a href=\"/https://arxiv.org/abs/1610.05820\">shadow traing</a>) ÁöÑÊúâÊïàÊäÄÊúØÔºåÂÖ∂Ë°®Á§∫Â¶Ç‰∏ãÂõæÊâÄÁ§∫Ôºö<br>\n<img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/Shadow%20model.png\" alt=\"ÂΩ±Â≠êËÆ≠ÁªÉÁöÑÂõæÁ§∫\"></p>\n<p>ÊîªÂáªËÄÖÈ¶ñÂÖà‰ΩøÁî®ÂΩ±Â≠êËÆ≠ÁªÉÊï∞ÊçÆÈõÜÂíåÂ≠¶‰π†ÁÆóÊ≥ïÊù•ËÆ≠ÁªÉËøô‰∫õÂΩ±Â≠êÊ®°ÂûãÔºå‰ΩøÂÆÉ‰ª¨ÁöÑË°å‰∏∫Â∞ΩÂèØËÉΩÊé•ËøëÁõÆÊ†áÊ®°Âûã„ÄÇÂÖ∑‰ΩìÁöÑËÆ≠ÁªÉÊñπÊ≥ïÂ¶Ç‰∏ãÔºö</p>\n<p>1.<strong> ËÆ≠ÁªÉÂΩ±Â≠êÊ®°Âûã</strong>ÔºöÊîªÂáªËÄÖ‰ΩøÁî®Â§ö‰∏™‰∏éÁõÆÊ†áËÆ≠ÁªÉÈõÜÂàÜÂ∏ÉÁõ∏ÂêåÂΩ±Â≠êËÆ≠ÁªÉÊï∞ÊçÆÈõÜÂíåÂ≠¶‰π†ÁÆóÊ≥ïËÆ≠ÁªÉÂΩ±Â≠êÊ®°Âûã<br>\n 2.<strong> Êî∂ÈõÜÈ¢ÑÊµãÂêëÈáè</strong>ÔºöÊîªÂáªËÄÖ‰ºö‰ΩøÁî®ÂΩ±Â≠êËÆ≠ÁªÉÊï∞ÊçÆÈõÜÂíåÂΩ±Â≠êÊµãËØïÊï∞ÊçÆÈõÜÂØπÂΩ±Â≠êÊ®°ÂûãËøõË°åÊü•ËØ¢ÔºåËé∑ÂèñÊØèÊù°Êï∞ÊçÆËÆ∞ÂΩïÁöÑÈ¢ÑÊµãÂêëÈáè„ÄÇÂΩ±Â≠êËÆ≠ÁªÉÊï∞ÊçÆÈõÜÁöÑÊØèÊù°ËÆ∞ÂΩïÁöÑÈ¢ÑÊµãÂêëÈáèË¢´Ê†áËÆ∞‰∏∫ ‚ÄúÊàêÂëò‚ÄùÔºåÂΩ±Â≠êÊµãËØïÊï∞ÊçÆÈõÜÁöÑÊØèÊù°ËÆ∞ÂΩïÁöÑÈ¢ÑÊµãÂêëÈáèË¢´Ê†áËÆ∞‰∏∫ ‚ÄúÈùûÊàêÂëò‚Äù„ÄÇ<br>\n3.<strong> ÊûÑÂª∫ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ</strong>ÔºöÊ†πÊçÆÁ¨¨‰∫åÈÉ®Ê†áËÆ∞ÁöÑÊï∞ÊçÆÊûÑÈÄ† ‚ÄúÊàêÂëò‚Äù Âíå ‚ÄúÈùûÊàêÂëò‚Äù ÁöÑÊï∞ÊçÆÈõÜ<br>\n 4.<strong> ËÆ≠ÁªÉÊîªÂáªÊ®°Âûã</strong>ÔºöËØÜÂà´ËÆ≠ÁªÉÊï∞ÊçÆÈõÜÊàêÂëòÂíåÈùûÊàêÂëòÁöÑÂ§çÊùÇÂÖ≥Á≥ªÈóÆÈ¢òË¢´ËΩ¨Êç¢‰∏∫‰∫åÂàÜÁ±ªÈóÆÈ¢òÔºåÂπ∂ÈÄöËøáÊú∫Âô®Â≠¶‰π†Ê°ÜÊû∂ËøõË°åÂ≠¶‰π†„ÄÇ</p>\n<p>Áé∞ÊúâÁöÑÂØπ‰∫é‰∫åÂÖÉÂàÜÁ±ªÂô®ÁöÑÁ†îÁ©∂ÂêåÊ†∑ÂàÜ‰∏∫ÈªëÁõíÊîªÂáªÂíåÁôΩÁõíÊîªÂáªÔºåÂÖ∑‰ΩìÂèØÂèÇËßÅÂºÄÂ§¥ÊèêÂà∞ÁöÑÊñáÁ´†Ôºå‰∏ªË¶ÅÈÄöËøáËÆ≠ÁªÉÂàÜÁ±ªÂáΩÊï∞‰ª•Ê≠§ËÆ°ÁÆóÊçüÂ§±Áéá„ÄÇ‰∫åËÄÖÂå∫Âà´Â§ßËá¥ÂèØÂèÇËßÅ‰∏ãÂõæÔºö<br>\n<img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/Binary_classifier.png\" alt=\"\"></p>\n<h3 id=\"Âü∫‰∫éÊåáÊ†áÁöÑmia-metric-based-membership-inference-attacks\"><a class=\"markdownIt-Anchor\" href=\"#Âü∫‰∫éÊåáÊ†áÁöÑmia-metric-based-membership-inference-attacks\">#</a> Âü∫‰∫éÊåáÊ†áÁöÑ MIA (Metric Based Membership Inference Attacks)</h3>\n<p>Âü∫‰∫éÊåáÊ†áÁöÑ MIA ÈÄöËøáÂØπÈ¢ÑÊµãÂêëÈáèËøõË°åÊî∂ÈõÜÂàÜÊûêÂπ∂Ëé∑ÂèñÁõ∏ÂÖ≥ÁöÑÊåáÊ†áÔºåÂπ∂ÈÄöËøáÊØîÂØπÊåáÊ†áÂíåÈòàÂÄºËøõË°åÂàÜÊûêÔºåÁõ∏ÊØîËÆ≠ÁªÉ‰∫åÂÖÉÂàÜÁ±ªÂô®ÔºåÂÖ∂Êõ¥‰∏∫ÁÆÄÂçïÂπ∂‰∏îÊ∂àËÄóËæÉÂ∞ëÁöÑËÆ°ÁÆóËµÑÊ∫êÔºåÁé∞‰ªäÂØπÊåáÊ†áÁöÑÂàÜÊûê‰ª•ÂèäËÆæÂÆö‰∏ªË¶ÅÂü∫‰∫é‰ª•‰∏ãÊñπÈù¢Ôºö</p>\n<p>1.<strong> Âü∫‰∫éÈ¢ÑÊµãÊ≠£Á°ÆÊÄßÁöÑÊîªÂáª (Prediction Correctness Based MIA)</strong>ÔºöÂ¶ÇÊûúÁõÆÊ†áÊ®°ÂûãÊ≠£Á°ÆÈ¢ÑÊµã‰∫ÜËæìÂÖ•ËÆ∞ÂΩï xÔºåÈÇ£‰πàÊîªÂáªËÄÖÂàôÂà§Êñ≠ÂÖ∂‰∏∫ÊàêÂëòÔºåÂê¶ÂàôÂ∞ÜÂÖ∂Âà§Êñ≠‰∏∫ÈùûÊàêÂëò„ÄÇ‰ªéÁõ¥Ëßâ‰∏äÁúãÔºåËæìÂÖ•Êï∞ÊçÆÂ¶ÇÊûúÂú®ÁúüÂÆûÊï∞ÊçÆ‰∏≠ÔºåÈÇ£‰πàÁõÆÊ†áÊ®°ÂûãÂ∞±ÂõûÊ≠£Á°ÆÈ¢ÑÊµãËæìÂÖ•ËÆ∞ÂΩï x„ÄÇ<br>\n2.<strong> Âü∫‰∫éÈ¢ÑÊµãÊçüÂ§±ÁöÑÊîªÂáª (Prediction Correctness Based MIA)</strong>ÔºöÂ¶ÇÊûúËæìÂÖ•Êüê‰∏™ËÆ∞ÂΩïÂØπÂ∫îÁöÑÊçüÂ§±Áéá‰∏éÂéüÂßãÊï∞ÊçÆÂ∑ÆË∑ùÂ∞è‰∫éÊüê‰∏™ÈòàÂÄºÔºåÈÇ£‰πàÂ∞±Âà§Êñ≠ÂÖ∂‰∏∫ÊàêÂëòÔºåÂèç‰πãÂàôÂê¶„ÄÇ‰ªéÁõ¥Ëßâ‰∏äÁúãÔºåËæìÂÖ•Êï∞ÊçÆÂ¶ÇÊûúÂú®ÁúüÂÆûÊï∞ÊçÆ‰∏≠ÔºåÈÇ£‰πàÁõÆÊ†áÊ®°ÂûãÁöÑÊçüÂ§±ÁéáÂ∞±Â∫îËØ•Âú®Êï¥‰ΩìÁöÑÊçüÂ§±ÁéáÈôÑËøë<br>\n 3.<strong> Âü∫‰∫éÈ¢ÑÊµãÁΩÆ‰ø°Â∫¶ÁöÑÊîªÂáª (Prediction Confidence Based MIA)</strong>ÔºöÂ¶ÇÊûúËæìÂÖ•Êüê‰∏™ËÆ∞ÂΩïÂêéÁöÑÈ¢ÑÊµãÁΩÆ‰ø°Â∫¶Â§ß‰∫éÊüê‰∏™ÈòàÂÄºÔºåÂàôÂà§Êñ≠ÂÖ∂‰∏∫ÊàêÂëò„ÄÇ‰ªéÁõ¥Ëßâ‰∏äÁúãÔºåÁõÆÊ†áÊ®°ÂûãÂ∞ÜÊúÄÂ∞èÂåñÂÖ∂‰∏éÂÆûÈôÖÊ®°ÂûãÁöÑÂ∑ÆË∑ùÔºåÂõ†Ê≠§ÂÖ∂È¢ÑÊµãÁΩÆ‰ø°Â∫¶Â∫îÂΩìÊé•Ëøë 1„ÄÇ<br>\n4.<strong> Âü∫‰∫éÈ¢ÑÊµãÁÜµÁöÑÊîªÂáªÔºàPrediction Entropy Based MIAÔºâ</strong> Â¶ÇÊûúËæìÂÖ•Êüê‰∏™ËÆ∞ÂΩïÂêéÂÖ∂È¢ÑÊµãÁÜµÂ∞è‰∫éÊüê‰∏™ÈòàÂÄºÔºåÂàôÂà§Êñ≠ÂÖ∂‰∏∫ÊàêÂëò„ÄÇ‰ªéÁõ¥Ëßâ‰∏äÁúãÔºåÁõÆÊ†áÊ®°ÂûãÁöÑÂØπËØïÈ™åÈ¢ÑÊµãÁÜµÈÉΩ‰ºöÂ§ß‰∫éÂÖ∂ÂØπÊµãËØïÁöÑÈ¢ÑÊµãÁÜµ<br>\n 5.<strong> Âü∫‰∫é‰øÆÊîπÂêéÁöÑÈ¢ÑÊµãÁÜµÁöÑÊîªÂáªÔºàModified Prediction Entropy Based MIAÔºâ</strong>ÔºöÊúâËßÇÁÇπËÆ§‰∏∫Áé∞ÊúâÁöÑÈ¢ÑÊµãÁÜµÊú™ËÄÉËôë ground truth labelÔºåÂõ†ËÄåÂèØËÉΩÈÄ†ÊàêÊüê‰∫õÊï∞ÊçÆË¢´ËØØÂà§„ÄÇÂõ†Ê≠§Âú®<span class=\"exturl\" data-url=\"aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDMuMTA1OTU=\">ËøôÁØáÊñáÁ´†</span>‰∏≠‰ΩúËÄÖÂØπÈ¢ÑÊµãÁÜµÁöÑÁÆóÊ≥ïËøõË°å‰∫Ü‰∏Ä‰∫õ‰øÆÊîπ</p>\n<h2 id=\"Ê†πÊçÆÁõÆÊ†áÊ®°ÂûãÁöÑÁõ∏ÂÖ≥Á†îÁ©∂\"><a class=\"markdownIt-Anchor\" href=\"#Ê†πÊçÆÁõÆÊ†áÊ®°ÂûãÁöÑÁõ∏ÂÖ≥Á†îÁ©∂\">#</a> Ê†πÊçÆÁõÆÊ†áÊ®°ÂûãÁöÑÁõ∏ÂÖ≥Á†îÁ©∂</h2>\n<h3 id=\"Âú®ÂàÜÁ±ªÊ®°ÂûãÊñπÈù¢ÁöÑÁ†îÁ©∂\"><a class=\"markdownIt-Anchor\" href=\"#Âú®ÂàÜÁ±ªÊ®°ÂûãÊñπÈù¢ÁöÑÁ†îÁ©∂\">#</a> Âú®ÂàÜÁ±ªÊ®°ÂûãÊñπÈù¢ÁöÑÁ†îÁ©∂</h3>\n<p>Ëá™ Shokri Á≠â‰∫∫ÊèêÂá∫‰∫ÜËøô‰∏ÄÊîªÂáªÊñπÂºèÂêéÔºåÁõÆÂâçÊúâËÆ∏Â§öÈíàÂØπÊ≠§ÊñπÂêëÁöÑÁ†îÁ©∂„ÄÇSalem Á≠â‰∫∫ËÆ®ËÆ∫‰∫Ü MIA ÁöÑÂÅáËÆæÂπ∂Â∞ùËØïÊîæÂÆΩ‰∫ÜÂÆûÁé∞ÁöÑÊù°‰ª∂ÔºåËØÅÊòé‰∫Ü shadow model ÁöÑ‰∏§ÁßçÂÅáËÆæÂπ∂ÈùûÂøÖÈ°ªÂπ∂ÊèêÂá∫‰∫ÜÂü∫‰∫éÊåáÊ†áÁöÑ MIA ÊñπÂºè„ÄÇYeom Á≠â‰∫∫‰πüÊèêÂá∫‰∫Ü‰∏§ÁßçÂü∫‰∫éÊåáÊ†áÁöÑ MIA ÊñπÂºèÔºõLong Á≠â‰∫∫ÈÄöËøáË∞ÉÂÖ≥Ê≥®Êüê‰∫õÂØπÁõÆÊ†áÊ®°ÂûãÊúâÁã¨ÁâπÂΩ±ÂìçÁöÑÊï∞ÊçÆÔºå‰ª•Ê≠§ÂÆûÁé∞ÂØπÊüê‰∫õÊï∞ÊçÆÁöÑ MIA ÊîªÂáªÔºåÂÆûÁé∞‰∫ÜÂú®ËÆ≠ÁªÉÂíåÊµãËØïÂáÜÁ°ÆÂ∫¶Áõ∏ËøëÁöÑÊÉÖÂÜµ‰∏ãÔºåÂú®ÈÄöÁî®ÂåñÁöÑÊ®°Âûã‰∏≠Ê≠£Á°ÆËøõË°åÊé®Êñ≠„ÄÇ</p>\n<p>Ê≠§Â§ñÔºåÁé∞ÊúâÁöÑÁ†îÁ©∂ÂêåÊ†∑‰πüÈíàÂØπÊõ¥Âä†ÂèóÈôêÁöÑ MIAÔºåLi Âíå Zhang ÊèêÂá∫‰∫ÜÂü∫‰∫éÂü∫‰∫éËΩ¨ÁßªÁöÑÔºàtransfer basedÔºâMIA ÂíåÂü∫‰∫éÊâ∞Âä®Ôºàperturbation basedÔºâÁöÑ MIA„ÄÇÂü∫‰∫éËΩ¨ÁßªÁöÑ MIA ÈÄöËøáÊûÑÈÄ†ÂΩ±Â≠êÊ®°ÂûãÊ®°ÊãüÁõÆÊ†áÊ®°ÂûãÔºåÁî®Èò¥ÂΩ±Ê®°ÂûãÁöÑÁΩÆ‰ø°Â∫¶Âà§Êñ≠ÊàêÂëòÔºõÂü∫‰∫éÊâ∞Âä®ÁöÑ MIA ÂàôÈÄöËøáÊ∑ªÂä†Âô™Èü≥Ôºå‰ΩøÂÖ∂ÂèòÊàêÂØπÊäóÊÄßÁöÑ‰æãÂ≠êÔºåÈÄöËøáÊâ∞Âä®ÁöÑ‰∏•ÈáçÁ®ãÂ∫¶Âå∫ÂàÜÊàêÂëò„ÄÇChoquette Á≠â‰∫∫ËøòÊèêÂá∫‰∫ÜÂü∫‰∫éÊï∞ÊçÆÂ¢ûÂº∫ÁöÑ MIA ÂíåÂü∫‰∫éÂÜ≥Á≠ñËæπÁïåË∑ùÁ¶ªÁöÑ MIA„ÄÇÂü∫‰∫éÊï∞ÊçÆÂ¢ûÂº∫ÁöÑÊîªÂáªÈíàÂØπÊú∫Âô®Â≠¶‰π†Á≥ªÁªü‰∏≠Â∏∏ËßÅÁöÑÊï∞ÊçÆÂ¢ûÂº∫Áé∞Ë±°ÔºåÈÄöËøá‰∏çÂêåÁöÑÊï∞ÊçÆÂ¢ûÂº∫Á≠ñÁï•ÂàõÂª∫È¢ùÂ§ñÁöÑÊï∞ÊçÆËÆ∞ÂΩïÔºå‰ª•Ê≠§Êü•ËØ¢ÁõÆÊ†áÊ®°ÂûãÊî∂ÈõÜÈ¢ÑÊµãÊ†áÁ≠æ„ÄÇÂü∫‰∫éÂÜ≥Á≠ñËæπÁïåÁöÑÊîªÂáªÁ≠ñÁï•Âàô‰º∞ËÆ°ËÆ∞ÂΩïÂà∞Ê®°ÂûãËæπÁïåÁöÑË∑ùÁ¶ªÔºåÂÖ∂Á±ª‰ºº Li Âíå zhang ÁöÑÊîªÂáª„ÄÇÁé∞ÊúâÁöÑ MIA ÊàêÂäüÊ°à‰æãË°®ÊòéÔºåÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÂèØËÉΩÊØîÊàë‰ª¨È¢ÑÊúüÁöÑÊõ¥ÂÆπÊòìÊî∂Âà∞ MIA ÁöÑÂΩ±Âìç„ÄÇ</p>\n<p>Èô§ÂéªÈíàÂØπ MIA ÁöÑÈªëÁõíÊîªÂáªÔºåNasir Á≠â‰∫∫È¶ñÊ¨°ÊèêÂá∫‰∫ÜÁôΩÁõí MIAÔºåÂÖ∂ÂèØÁúã‰ΩúÊòØÂü∫‰∫éÈªëÁõí MIA Âú®ËøõË°åÁöÑÁöÑÊãìÂ±ïÔºåÈÄöËøáËé∑ÂèñÁöÑÊõ¥Â§ö‰ø°ÊÅØÊèêÂçáÊîªÂáªÊïàÁéá„ÄÇ‰ªñ‰ª¨ÈááÁî®ÁõÆÊ†áÊ®°ÂûãÈ¢ÑÊµãÊçüÂ§±ÁöÑÊ¢ØÂ∫¶ËøõË°åÊé®Êñ≠Ôºå‰ª•Ê≠§ÈÄöËøá SGD ÁÆóÊ≥ïËÆ≠ÁªÉÂå∫ÂàÜÊàêÂëò‰∏éÈùûÊàêÂëò„ÄÇ‰ΩÜ Leino and Fredrikson ÊåáÂá∫ÂÖ∂ÊñπÊ≥ïÁöÑÂÅáËÆæËøá‰∫é‰∏•Ê†ºÔºåÂÖ∂ÈúÄË¶ÅÊîªÂáªËÄÖÁü•ÈÅìÁõÆÊ†áÊï∞ÊçÆÈõÜÁöÑÂ§ßËá¥ÂàÜÂ∏ÉÔºåÂõ†Ê≠§‰ªñ‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éË¥ùÂè∂ÊñØÊúÄ‰Ω≥ÊîªÂáªÔºàBayes-optimal attackÔºâÁöÑ MIA ÊñπÊ≥ïÔºå‰ªéËÄåÂÆûÁé∞Êó†È°ªÁõÆÊ†áÊ®°ÂûãËÉåÊôØÁü•ËØÜÁöÑ MIA„ÄÇ</p>\n<h3 id=\"Âú®ÁîüÊàêÊ®°Âûã‰∏äÁöÑÁ†îÁ©∂\"><a class=\"markdownIt-Anchor\" href=\"#Âú®ÁîüÊàêÊ®°Âûã‰∏äÁöÑÁ†îÁ©∂\">#</a> Âú®ÁîüÊàêÊ®°Âûã‰∏äÁöÑÁ†îÁ©∂</h3>\n<p>Áé∞ÊúâÁöÑÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠‰∫éÁîüÊàêÂºèÂØπÊäóÁ•ûÁªèÁΩëÁªú‰∏ä (generative adversarial network). ÂÖ∂Ê®°ÂûãÂ§ßËá¥Â¶Ç‰∏ãÂõæÂèØËßÅ<br>\n<img data-src=\"https://raw.githubusercontent.com/ckt1i/blogimage/main/GAN_architecture.png\" alt=\"\"></p>\n<p>Hayes Á≠â‰∫∫È¶ñÊ¨°ÊèêÂá∫‰∫ÜÂÖ≥‰∫éÁîüÊàêÊ®°ÂûãÁöÑ MIAÔºåÂØπ‰∫éÁôΩÁõíÊîªÂáªÔºåÊîªÂáªËÄÖÊî∂ÈõÜÊâÄÊúâËÆ∞ÂΩïÂπ∂ËÆ°ÁÆóÁΩÆ‰ø°Â∫¶ËøõË°åÂà§Âà´ÔºõÂØπ‰∫éÈªëÁõíÊîªÂáªÔºåÊîªÂáªËÄÖ‰ªéÁîüÊàêÂô®‰∏≠Êî∂ÈõÜËÆ∞ÂΩï‰ª•ËÆ≠ÁªÉÊú¨Âú∞ GAN ‰ª•Ê®°‰ªøÁõÆÊ†á GANÔºåÂπ∂ÈÄöËøá‰ΩøÁî®Êú¨Âú∞ GAN Èâ¥Âà´Âô®ËøõË°åÂà§Âà´„ÄÇHilprecht Á≠â‰∫∫ÊèêÂá∫‰∫ÜÂè¶Â§ñ‰∏§ÁßçÊîªÂáªÔºåÂàÜÂà´ÊòØÂü∫‰∫éËíôÁâπÂç°Ê¥õÁöÑÈªëÁõíÊîªÂáªÂíåÂü∫‰∫é VAE ÁöÑÁôΩÁõíÊîªÂáª„ÄÇHilpreche Á≠â‰∫∫ÊèêÂá∫‰∫ÜÂà§Êñ≠Êüê‰∏™Êï∞ÊçÆÊòØÂê¶Âú®ÈõÜÂêàÂÜÖÁöÑÈõÜÂêàÊîªÂáªÔºåLiu Á≠â‰∫∫ÂàôÊèêÂá∫‰∫Ü‰∏éÂÖ∂Áõ∏‰ººÁöÑ co-membership inferenceÔºåÈÄöËøáÂàÜÊûêÊüê‰∏™Êï∞ÊçÆÂà∞ÁõÆÊ†áÊï∞ÊçÆÁöÑË∑ùÁ¶ªÊù•Âà§Êñ≠ÊòØÂê¶Âú®Êï∞ÊçÆÈõÜÂÜÖ„ÄÇChen Á≠â‰∫∫ÊèêÂá∫‰∫Ü‰∏ÄÁßçÈÄöÁî®ÁöÑÊñπÊ≥ïÔºåÊîªÂáªËÄÖÈÄöËøáÊúÄ‰ºòÂåñÊñπÊ≥ï‰∏çÊñ≠ÈáçÂª∫ÊîªÂáªÊ®°ÂûãÔºåÂπ∂Ê†πÊçÆÊîªÂáªÊ®°ÂûãËÆ°ÁÆóÂÖ∂ÁîüÊàêÁªìÊûú‰∏éÁõÆÊ†áÊ®°ÂûãÁöÑË∑ùÁ¶ªÔºåÂπ∂‰∏îÈÄöËøáË∑ùÁ¶ª‰º∞ÁÆóÊï∞ÊçÆÂú®ÂÖ∂‰∏≠ÁöÑÊ¶ÇÁéá„ÄÇ</p>\n<h3 id=\"Âú®ÂµåÂÖ•Ê®°Âûã‰∏äÁöÑÁ†îÁ©∂\"><a class=\"markdownIt-Anchor\" href=\"#Âú®ÂµåÂÖ•Ê®°Âûã‰∏äÁöÑÁ†îÁ©∂\">#</a> Âú®ÂµåÂÖ•Ê®°Âûã‰∏äÁöÑÁ†îÁ©∂</h3>\n<p>Áé∞ÊúâÁöÑÁ†îÁ©∂‰∏ªË¶ÅÈíàÂØπÊñáÊú¨ÂíåÂõæÂÉèÁöÑÁ†îÁ©∂ÔºåÈíàÂØπÊñáÊú¨ÂµåÂÖ•Ê®°ÂûãÔºåÊîªÂáªÁöÑÁõÆÊ†áÊòØÊé®Êñ≠ÊªëÂä®Á™óÂè£ÁöÑËØçËØ≠ÊàñÂè•Â≠êÂØπÁöÑÊàêÂëòËµÑÊ†ºÔºåÂà©Áî®ÂÆÉ‰ª¨ÁöÑÁõ∏‰ººÊÄßÂàÜÊï∞Êù•Êé®Êñ≠ÂÆÉ‰ª¨ÊòØÂê¶Â±û‰∫éÊüê‰∏™È¢ÑÂÆö‰πâÁöÑÈõÜÂêà„ÄÇÂØπ‰∫éÂõæÂµåÂÖ•Ê®°ÂûãÔºåÊîªÂáªÊñπÊ≥ïÊ∂âÂèä‰ΩøÁî®Èò¥ÂΩ±Ê®°ÂûãÂíåÁΩÆ‰ø°Â∫¶ÂàÜÊï∞Êù•Êé®Êñ≠Âõæ‰∏≠ËäÇÁÇπÊòØÂê¶Â±û‰∫éÁâπÂÆöÁ±ªÂà´ÔºåÂç≥ËäÇÁÇπÂàÜÁ±ªÈóÆÈ¢ò„ÄÇ</p>\n<h3 id=\"Âú®ÂõûÂΩíÊ®°Âûã‰∏äÁöÑÁ†îÁ©∂\"><a class=\"markdownIt-Anchor\" href=\"#Âú®ÂõûÂΩíÊ®°Âûã‰∏äÁöÑÁ†îÁ©∂\">#</a> Âú®ÂõûÂΩíÊ®°Âûã‰∏äÁöÑÁ†îÁ©∂</h3>\n<p>Gupta Á≠â‰∫∫È¶ñÊ¨°ËøõË°å‰∫ÜÈíàÂØπÂπ¥ÈæÑÈ¢ÑÊµãÁöÑÂõûÂΩíÊ®°Âûã‰∏äÁöÑ MIA Á†îÁ©∂ÔºåÈÄöËøáÊûÑÈÄ†ÁôΩÁõíÁöÑ‰∫åÂÖÉÂà§Êñ≠Ê®°ÂûãÂÆûÁé∞ÊîªÂáª„ÄÇ</p>\n<h3 id=\"Âú®ËÅîÈÇ¶Â≠¶‰π†ÁöÑÁ†îÁ©∂\"><a class=\"markdownIt-Anchor\" href=\"#Âú®ËÅîÈÇ¶Â≠¶‰π†ÁöÑÁ†îÁ©∂\">#</a> Âú®ËÅîÈÇ¶Â≠¶‰π†ÁöÑÁ†îÁ©∂</h3>\n<p>Âú®ËÅîÈÇ¶Â≠¶‰π†‰∏≠ÔºåÊîªÂáªËÄÖÂèØ‰ª•ÊòØ‰∏≠ÂøÉÊúçÂä°Âô®ÊàñËÄÖÂÖ∂‰∏≠ÁöÑÊüê‰∫õÂàÜÊú∫ÔºåÈÄöËøáÂà§Êñ≠Êüê‰∫õÊï∞ÊçÆÊòØÂê¶Áî®‰∫éÂÖ®Â±ÄÊ®°ÂûãÁöÑËÆ≠ÁªÉ‰ª•ÂÆûÁé∞ MIA„ÄÇMelis ÈÄöËøáÂàÜÊûê RNN ËÆ≠ÁªÉÂ∏àÂµåÂÖ•Â±ÇÁöÑÊõ¥Êñ∞Êú∫Âà∂ÔºåÈ¶ñÊ¨°ÊèêÂá∫‰∫ÜÂü∫‰∫éÊ¢ØÂ∫¶ÁöÑ MIA„ÄÇTurex ÂàôÊèêÂá∫‰∫ÜÂºÇÊûÑ FLÔºàheterogeneous FLÔºâÔºåÈÄöËøáÂàÜÊûê‰∏çÂêåÂàÜÊú∫Ê±áÊÄªÂèÇÊï∞ÁöÑÂ∑ÆÂºÇËøõË°åÂà§Êñ≠„ÄÇNasr Á≠â‰∫∫ËÆ®ËÆ∫‰∫ÜÂ¶Ç‰ΩïÈÄöËøáÊ¢ØÂ∫¶‰∏äÂçáÊîªÂáª‰∏ªÂä®Âπ≤È¢Ñ FL ËÆ≠ÁªÉ„ÄÇHu Á≠â‰∫∫ÂàôÊèêÂá∫‰∫ÜÊ∫êÊé®Êñ≠ÊîªÂáªÔºåÊó®Âú®Á°ÆÂÆöÂì™‰∏™ÂèÇ‰∏éÊñπÊã•Êúâ FL ‰∏≠ÁöÑËÆ≠ÁªÉËÆ∞ÂΩï„ÄÇ‰ªñ‰ª¨ËÆ§‰∏∫Áé∞ÊúâÁöÑ FL ‰∏≠ÁöÑÊàêÂëòÊé®Êñ≠ÊîªÂáªÂøΩËßÜ‰∫ÜËÆ≠ÁªÉÊàêÂëòÁöÑÊù•Ê∫ê‰ø°ÊÅØÔºåËÄåËøô‰∫õ‰ø°ÊÅØÁöÑÊ≥ÑÈú≤ÂèØËÉΩÂØºËá¥Ëøõ‰∏ÄÊ≠•ÁöÑÈöêÁßÅÈóÆÈ¢ò„ÄÇ</p>\n<h1 id=\"mia-ÊîªÂáªÊàêÂäüÁöÑÂõ†Á¥†\"><a class=\"markdownIt-Anchor\" href=\"#mia-ÊîªÂáªÊàêÂäüÁöÑÂõ†Á¥†\">#</a> MIA ÊîªÂáªÊàêÂäüÁöÑÂõ†Á¥†</h1>\n<h2 id=\"ÁõÆÊ†áÊ®°ÂûãÂØπÂéüÂßãÊï∞ÊçÆÈõÜÁöÑËøáÊãüÂêà\"><a class=\"markdownIt-Anchor\" href=\"#ÁõÆÊ†áÊ®°ÂûãÂØπÂéüÂßãÊï∞ÊçÆÈõÜÁöÑËøáÊãüÂêà\">#</a> ÁõÆÊ†áÊ®°ÂûãÂØπÂéüÂßãÊï∞ÊçÆÈõÜÁöÑËøáÊãüÂêà</h2>\n<p>ËØ•ÁªºËø∞Ë°®ÊòéÔºåÁõÆÂâçÂ∑≤ÊúâËÆ∏Â§öÁ†îÁ©∂ÊåáÂá∫ target ML models ÂØπÁõÆÊ†áÁöÑËøáÊãüÂêàÊòØÈÄ†ÊàêÂéüÂßãÊï∞ÊçÆÈõÜÊ≥ÑÊºèÁöÑÈáçË¶ÅÂõ†Á¥†ÔºåÂÖ∑‰ΩìÊúâÂ¶Ç‰∏ãÂéüÂõ†Ôºö</p>\n<ol>\n<li>DNN Á≠âÊ®°ÂûãÂú®Â∫îÁî®‰∏≠ÁöÑËøáÂèÇÊï∞Âåñ‰∏ÄÊñπÈù¢ÊèêÂçá‰∫ÜÂ§ÑÁêÜÂ§ßÊï∞ÊçÆÁöÑËÉΩÂäõÔºåÂè¶‰∏ÄÊñπÈù¢‰πüËÆ∞ÂΩï‰∫ÜÂ§ßÈáèÊï∞ÊçÆÁöÑÊó†Êïà‰ø°ÊÅØ„ÄÇ</li>\n<li>Âú®ËÆ≠ÁªÉÊú∫Âô®Â≠¶‰π†Êó∂ÈÄöÂ∏∏ÈúÄË¶ÅËæÉÂ§ö epochÔºå‰ΩøÂÖ∂Êõ¥ÂÄæÂêë‰∫éËÆ∞ÂøÜÊï∞ÊçÆÈõÜ‰∏≠ÁöÑÂÜÖÂÆπ</li>\n<li>Êú∫Âô®Â≠¶‰π†ÁöÑÊï∞ÊçÆÈõÜÊó†Ê≥ïÂÆåÂÖ®‰ª£Ë°®ÂÆûÈôÖÊï∞ÊçÆÈõÜ</li>\n</ol>\n<p>ÂØπ‰∫é‰∏Ä‰∏™ÂØπËÆ≠ÁªÉÊï∞ÊçÆËøáÊãüÂêàÁöÑÂàÜÁ±ªÁ≥ªÁªüÔºåÊîªÂáªËÄÖÂèØ‰ª•Âü∫‰∫éÈöèÊú∫ÁåúÊµãÁöÑÈ¢ÑÊµãÊ≠£Á°ÆÊÄßÂÆûÁé∞È´ò‰∫é 50% ÁöÑÊîªÂáªÊàêÂäüÊ¶ÇÁéáÔºåËøôÁÇπÁöÑËØÅÊòéÂèØ‰ª•Âú®<a href=\"/https://arxiv.org/abs/2009.05669\">ËøôÁØáÊñáÁ´†</a>ÂèØËßÅ</p>\n<h2 id=\"ËÆ≠ÁªÉÊ®°ÂûãËá™Ë∫´ÁöÑÁâπÂæÅ\"><a class=\"markdownIt-Anchor\" href=\"#ËÆ≠ÁªÉÊ®°ÂûãËá™Ë∫´ÁöÑÁâπÂæÅ\">#</a> ËÆ≠ÁªÉÊ®°ÂûãËá™Ë∫´ÁöÑÁâπÂæÅ</h2>\n<p>ÂΩìÁõÆÊ†áÊ®°ÂûãÁöÑÂÜ≥Á≠ñËæπÁïåÂØπ‰∫éÊâÄ‰ΩøÁî®ÁöÑËÆ≠ÁªÉÊï∞ÊçÆÂπ∂‰∏çÊïèÊÑüÁöÑÊó∂ÂÄôÔºåMIA ÊîªÂáªÁöÑÊúâÊïàÊÄß‰∏çÈ´ò„ÄÇ<a href=\"/https://ieeexplore.ieee.org/document/8962136\">ËøôÁØáÊñáÁ´†</a>ÂÆûÈ™åÊï∞ÊçÆË°®ÊòéÔºåÂú® DNN models, logistic regression models, Naive Bayes models, k-nearest neighbor models, and decision tree models ÂØπÂÜ≥Á≠ñÊ†ëÊ®°ÂûãÂÖ∑ÊúâÊúÄÈ´òÁöÑÊîªÂáªÁ≤æÂ∫¶ÔºåËÄåÁÆÄÂçïË¥ùÂè∂ÊñØÁÆóÊ≥ï (Naive Bayes) ÂÖ∑ÊúâÊúÄ‰ΩéÁöÑÊîªÂáªÁ≤æÂ∫¶</p>\n<h2 id=\"ËÆ≠ÁªÉÊï∞ÊçÆÈõÜÁöÑÂ§öÊ†∑ÊÄß\"><a class=\"markdownIt-Anchor\" href=\"#ËÆ≠ÁªÉÊï∞ÊçÆÈõÜÁöÑÂ§öÊ†∑ÊÄß\">#</a> ËÆ≠ÁªÉÊï∞ÊçÆÈõÜÁöÑÂ§öÊ†∑ÊÄß</h2>\n<p>ÂΩìÁõÆÊ†áÊ®°Âûã‰ΩøÁî®ÁöÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÂÖ∑ÊúâËæÉÂº∫ÁöÑÂ§öÊ†∑ÊÄßÁöÑÊó∂ÂÄôÔºåËÆ≠ÁªÉÊï∞ÊçÆÂ∞ÜÂ∏ÆÂä©Ê®°ÂûãÊõ¥Â•ΩÁöÑÊ¶ÇÊã¨ÊµãËØïÊï∞ÊçÆ„ÄÇÂõ†Ê≠§ MIA ÂØπËØ•Ê®°ÂûãÁöÑÂΩ±ÂìçÂ∞±‰ºöË∂äÂ∞è„ÄÇ</p>\n<h2 id=\"ÊîªÂáªËÄÖÂØπÁõÆÊ†áÊ®°ÂûãÁöÑ‰∫ÜËß£Á®ãÂ∫¶\"><a class=\"markdownIt-Anchor\" href=\"#ÊîªÂáªËÄÖÂØπÁõÆÊ†áÊ®°ÂûãÁöÑ‰∫ÜËß£Á®ãÂ∫¶\">#</a> ÊîªÂáªËÄÖÂØπÁõÆÊ†áÊ®°ÂûãÁöÑ‰∫ÜËß£Á®ãÂ∫¶</h2>\n<p>Áé∞ÊúâÁöÑÈíàÂØπ MIA ÁöÑÁ†îÁ©∂Âü∫Êú¨ÂØπÊîªÂáªËÄÖÈÉΩÊúâ‰∏ÄÂÆöÁöÑÂÅáËÆæÔºö ÊîªÂáªËÄÖÁü•ÈÅìËÆ≠ÁªÉÊï∞ÊçÆÁöÑÁõ∏ÂÖ≥ÂàÜÂ∏ÉÔºåÂπ∂‰∏îÂèØ‰ª•Ê†πÊçÆÁõ∏ÂÖ≥ÁöÑÂàÜÂ∏ÉÊûÑÈÄ†Âá∫ÈÄÇÂêàÁöÑÂΩ±Â≠êÊï∞ÊçÆÈõÜ„ÄÇÂü∫‰∫éËøô‰∏ÄÂÅáËÆæÊûÑÈÄ†ÁöÑÈ´òÁ≤æÁ°ÆÂ∫¶ÁöÑÂΩ±Â≠êÊ®°ÂûãÊâçÂèØ‰ª•ÊúâÊïàÂú∞ÂÆûÁé∞ÊîªÂáª„ÄÇ</p>\n<h1 id=\"mia-ÁöÑÈò≤Âæ°Á†îÁ©∂\"><a class=\"markdownIt-Anchor\" href=\"#mia-ÁöÑÈò≤Âæ°Á†îÁ©∂\">#</a> MIA ÁöÑÈò≤Âæ°Á†îÁ©∂</h1>\n<h2 id=\"ÂèØ‰ø°ÂæóÂàÜÊé©Á†Åconfidence-score-masking\"><a class=\"markdownIt-Anchor\" href=\"#ÂèØ‰ø°ÂæóÂàÜÊé©Á†Åconfidence-score-masking\">#</a> ÂèØ‰ø°ÂæóÂàÜÊé©Á†Å Confidence Score Masking</h2>\n<p>Ê≠§ÊñπÊ≥ï‰∏ªË¶ÅÁî®‰∫éÈªëÁõíÊîªÂáªÁöÑÈò≤Âæ°ÔºåÈÄöËøáÂêëÂàÜÁ±ªÂô®ËøîÂõûÈöêËóèÂêéÁöÑÁúüÂÆûÁöÑÂèØ‰ø°ÂæóÂàÜ‰ª•ÂÆûÁé∞Èò≤Âæ°ÔºåÂÖ∑‰ΩìÊúâ‰ª•‰∏ã‰∏âÁßçÊñπÂºèÔºö</p>\n<ol>\n<li>target classifier Âπ∂‰∏çÊèê‰æõÂÆåÊï¥ÁöÑÈ¢ÑÊµãÂêëÈáèÔºåËÄåÊòØÂè™Êèê‰æõÊúÄÈ´òÁöÑÂá† confedence score</li>\n<li>target classifier Âè™Âú®ÊîªÂáªËÄÖÊèê‰æõÊï∞ÊçÆËæìÂÖ•Êó∂Êèê‰æõÈ¢ÑÊµãÁöÑÊ†áÁ≠æ</li>\n<li>Â∞ÜÂô™Â£∞Ê∑ªÂä†Âà∞ËøîÂõûÂêëÈáè‰∏ä<br>\nÊ≠§‰∏âÁßçÊñπÊ≥ïÂè™ÂΩ±ÂìçÈ¢ÑÊµãÂêëÈáèËÄå‰∏ç‰ºöÈÄ†ÊàêÈ¢ÑÊµãÁ≤æÁ°ÆÂ∫¶ÁöÑÊçüÂ§±</li>\n</ol>\n<h2 id=\"Ê≠£ÂàôÂåñ\"><a class=\"markdownIt-Anchor\" href=\"#Ê≠£ÂàôÂåñ\">#</a> Ê≠£ÂàôÂåñ</h2>\n<p>Ê≠£ÂàôÂåñÈÄöËøáÈôç‰ΩéÊ®°ÂûãÁöÑËøáÊãüÂêàÁ®ãÂ∫¶‰ª•ÂáèËΩª MIA ÁöÑÊîªÂáªÂº∫Â∫¶„ÄÇÁé∞ÊúâÁöÑÊ≠£ÂàôÂåñÊ®°ÂûãÂåÖÊã¨‰ª•‰∏ãÂá†ÁßçÔºö‰º†ÁªüÁöÑÊ≠£ÂàôÂåñÊñπÂºèÊòØÔºöL2-norm regularization, dropout , data argumentation, model stacking, early stopping, label smoothing. ÂÖ∂ÈÄöËøáÈôç‰Ωé‰∏çÂêåÊµãËØïÊï∞ÊçÆÈõÜÂØπÊ†∑Êú¨ÁöÑÂΩ±Âìç‰ª•Èôç‰ΩéËøáÊãüÂêàÁ®ãÂ∫¶ÔºåÂêåÊó∂‰πüÂèØ‰ª•ÂáèËΩªÂØπ MIA ÊîªÂáªÁöÑÂº∫Â∫¶„ÄÇÊ≠§Â§ñÔºåÂ¶Ç adversarial regular- ization , and Mixup + MMD (Maximum Mean Discrepancy) Ëøô‰∏§ÁßçÁâπÂà´ËÆæËÆ°ÁöÑÊ≠£ÂàôÂåñÁ≥ªÁªüÂêåÊ†∑‰πüÂèØ‰ª•Èò≤Âæ° MIAÔºåÈÄöËøáÂæÄÁõÆÊ†áÂàÜÁ±ªÂô®‰∏≠Ê∑ªÂä†Êñ∞ÁöÑÊ≠£ÂàôÂåñÊú∫Âà∂‰ª•Èôç‰ΩéÊàêÂëòÂíåÈùûÊàêÂëò‰πãÈó¥ÁöÑÂ∑ÆÂºÇ<br>\nÁõ∏ÊØî‰∫éÊé©Á†ÅÊäÄÊúØÔºåÊ≠£ÂàôÂåñÂèØ‰ª•ÊäµÊäóÈªëÁõíÂíåÁôΩÁõíÊîªÂáªÔºåÂÖ∂Âú®‰øÆÊîπËæìÂá∫Ê®°ÂûãÁöÑÊó∂ÂÄô‰πüÂèØ‰ª•ÊîπÂèòËæìÂá∫ÁöÑÂèÇÊï∞</p>\n<h2 id=\"Áü•ËØÜËí∏È¶èknowledge-distillation\"><a class=\"markdownIt-Anchor\" href=\"#Áü•ËØÜËí∏È¶èknowledge-distillation\">#</a> Áü•ËØÜËí∏È¶è (Knowledge Distillation)</h2>\n<p>Áü•ËØÜËí∏È¶èÊåáÈÄöËøáÂ§ßÂûãÊïôÂ∏àÊ®°ÂûãËÆ≠ÁªÉÂ∞èÂûãÂ≠¶ÁîüÊ®°ÂûãÔºå‰ª•Ê≠§Â∞ÜÁü•ËØÜ‰ªéÂ§ßÊ®°Âûã‰∏≠‰º†ËæìÂà∞Â∞èÊ®°Âûã‰∏≠ÂéªÔºå‰ΩøÂ∞èÊ®°ÂûãËÉΩÂ§üËé∑ÂæóÁõ∏ËøëÁöÑËøë‰ººÁ®ãÂ∫¶„ÄÇÂü∫‰∫éÊ≠§ÔºåÁé∞ÊúâÁöÑÁ†îÁ©∂Êèê‰æõ‰∫Ü DMP ‰ª•Âèä CKD Âíå PCKD ÊñπÊ≥ïÔºö</p>\n<p><a href=\"/https://arxiv.org/abs/1906.06589\">DMPÔºàDistillation For Membership PrivacyÔºâÊñπÊ≥ïÔºö</a>ÈÄöËøá‰∏Ä‰∏™ÁßÅÊúâÁöÑÊï∞ÊçÆÈõÜÂíåÂèÇÊï∞Êï∞ÊçÆÈõÜËøõË°åÈò≤Âæ°ÔºåÂÖ∑‰ΩìÊ≠•È™§Â¶Ç‰∏ã:</p>\n<ol>\n<li>ËÆ≠ÁªÉ‰∏Ä‰∏™Êó†‰øùÊä§ÁöÑÊïôÂ∏àÊ®°ÂûãÔºåÂπ∂‰ª•Ê≠§Âú®Êú™Ê†áÁ≠æÁöÑÊï∞ÊçÆÈõÜ‰∏≠ËøõË°åËÆ∞ÂΩïÂπ∂Ê†áËÆ∞</li>\n<li>ÈÄâÂèñÂÖ∂‰∏≠È¢ÑÊµãÁÜµËæÉ‰ΩéÁöÑÊï∞ÊçÆËøõË°åËÆ≠ÁªÉÔºåËøô‰∫õÊï∞ÊçÆ‰ª•‰∏∫ÂàÜÁ±ª</li>\n<li>Âü∫‰∫éÂ∑≤Ê†áËÆ∞ÁöÑÊ®°ÂûãËøõË°åËÆ≠ÁªÉ„ÄÇ</li>\n</ol>\n<p>Ê≠§Â§ñÔºå<a href=\"/https://www.sciencedirect.com/science/article/abs/pii/S0925231221006329\">Ëøô‰∏ÄÁØáËÆ∫Êñá</a>ÊèêÂá∫‰∫Ü‰∫íË°•Áü•ËØÜËí∏È¶èÔºàComplementary Knowledge DistillationÔºåCKDÔºâÂíå‰º™‰∫íË°•Áü•ËØÜËí∏È¶èÔºàPseudo Complementary Knowledge DistillationÔºåPCKDÔºâÊñπÊ≥ï„ÄÇÂú®Ëøô‰∫õÊñπÊ≥ï‰∏≠ÔºåÁü•ËØÜËí∏È¶èÁöÑËΩ¨ÁßªÊï∞ÊçÆÈÉΩÊù•Ëá™ÁßÅÊúâËÆ≠ÁªÉÈõÜ„ÄÇCKD Âíå PCKD Ê∂àÈô§‰∫ÜÂú®Êüê‰∫õÂ∫îÁî®‰∏≠ÂèØËÉΩÈöæ‰ª•Ëé∑ÂæóÁöÑÂÖ¨ÂÖ±Êï∞ÊçÆÁöÑÈúÄÊ±ÇÔºå‰ΩøÁü•ËØÜËí∏È¶èÊàê‰∏∫‰∏ÄÁßçÊõ¥ÂÆûÁî®ÁöÑÈò≤Âæ°ÊñπÊ≥ïÊù•ÂáèËΩªÊú∫Âô®Â≠¶‰π†Ê®°Âûã‰∏äÁöÑ MIA ÊîªÂáª„ÄÇ</p>\n<h2 id=\"Â∑ÆÂàÜÈöêÁßÅdifferential-privacy\"><a class=\"markdownIt-Anchor\" href=\"#Â∑ÆÂàÜÈöêÁßÅdifferential-privacy\">#</a> Â∑ÆÂàÜÈöêÁßÅ (Differential Privacy)</h2>\n<p>Â∑ÆÂàÜÈöêÁßÅÊåáÁöÑÊòØÈÄöËøáÂêëÂéüÂßãÊï∞ÊçÆÈõÜ‰∏≠Ê∑ªÂä†Áõ∏ÂÖ≥ÁöÑÊâ∞Âä®Êï∞Ôºå‰ª•ÂÆûÁé∞ÂØπÂéüÂßãÊï∞ÊçÆÁöÑ‰øùÊä§ÔºåÂΩì‰∏Ä‰∏™Ê∑±Â∫¶Â≠¶‰π†Ê®°Âûã‰ΩøÁî®‰∫ÜÂ∑ÆÂàÜÈöêÁßÅÂêéÁöÑÊ®°ÂûãËøõË°åËÆ≠ÁªÉÊó∂ÔºåÂ¶ÇÊûúÂÖ∂ÈöêÁßÅÈ¢ÑÁÆóÂ§üÂ∞èÔºåÈÇ£‰πàÂ≠¶‰π†ÂêéÁöÑÊ®°ÂûãÂπ∂‰∏ç‰ºöËÆ∞‰ΩèÁî®Êà∑ÁöÑÁõ∏ÂÖ≥‰ø°ÊÅØ„ÄÇÁî±Ê≠§Ôºå‰∏çÂêåÁöÑÈöêÁßÅÊ®°ÂûãÂ∞±ÂèØ‰ª•ÈôêÂà∂‰ªÖÂü∫‰∫éÊ®°ÂûãÁöÑ MIA ÊàêÂäüÂá†Áéá„ÄÇÁé∞ÊúâÁöÑÁ†îÁ©∂ËøõÂ±ï‰∏ªË¶ÅÈõÜ‰∏≠Âú®‰ª•‰∏ãÊñπÈù¢Ôºö</p>\n<ol>\n<li>Â∑ÆÂàÜÈöêÁßÅ‰∏é MIA ÁöÑÂÖ≥Á≥ªÔºåËøô‰∏ÄÊñπÈù¢Â∑≤ÊúâÁõ∏ÂÖ≥ÁöÑÁêÜËÆ∫ÂíåËØÅÊòéÔºå‰ΩÜÂú®ÂÆûÈôÖÂ∫îÁî®‰∏äÁöÑËØÑ‰º∞Âπ∂Êú™ÂèñÂæóËæÉÂ•ΩÁöÑÊïàÁî®</li>\n<li>ÈöêÁßÅ - ÊïàÁî®ÊùÉË°°ÔºöÁé∞ÊúâÁöÑÁ†îÁ©∂Ë°®ÊòéÂΩìÂâçÁöÑÂ∑ÆÂàÜÈöêÁßÅÂú®Ëøô‰∏ÄÊñπÈù¢ÊÄßËÉΩ‰∏çÂ§üÂ•ΩÔºåÁõ∏ÂÖ≥ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂ∞ëÊï∞Áæ§‰ΩìÊõ¥ÊòìÂèó MIAs ÂΩ±ÂìçÔºå‰∏îÂ∑ÆÂàÜÈöêÁßÅÈôç‰Ωé‰∫ÜËøô‰∫õÁæ§‰ΩìÁöÑÊ®°ÂûãÊïàÁî®„ÄÇ</li>\n<li>ËÆ≠ÁªÉÊñπÊ≥ïÔºöÁé∞ÊúâÁöÑÊñπÊ≥ï‰∏ªË¶ÅÊòØ DP-SGDÔºåÁé∞Âú®‰πüÊúâ DP-Logits Á≠âÊñ∞ÊñπÊ≥ïË¢´ÊèêÂá∫</li>\n<li>ÁîüÊàêÊ®°Âûã‰∏≠ÁöÑÂ∫îÁî®ÔºöÁé∞ÊúâÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂ∑ÆÂàÜÈöêÁßÅ‰πüÂèØ‰ª•Áî®‰∫éÈò≤Âæ°ÁîüÊàêÊ®°Âûã‰∏≠ÁöÑ MIAsÔºåÂÖ∂Èò≤Âæ°ÊïàÊûú‰∏éÁîüÊàêË¥®Èáè‰∏éÈöêÁßÅÈ¢ÑÁÆóùúñÁõ∏ÂÖ≥„ÄÇÂπ∂ÊúâÁ†îÁ©∂Ë°®Êòé DP ÂêÑÂºÇÈôêÂà∂ËøáÊãüÂêàÔºåÂáèËΩª MIA</li>\n</ol>\n<p>DP ‰∏∫‰øùÊä§ËÆ≠ÁªÉËÆ∞ÂΩïÁöÑÊàêÂëòÈöêÁßÅÊèê‰æõ‰∫ÜÁêÜËÆ∫‰øùÈöúÔºåÂèØ‰ª•Áî®‰∫éÁºìËß£ÂàÜÁ±ªÊ®°ÂûãÂíåÁîüÊàêÊ®°Âûã‰∏≠ÁöÑ MIAsÔºåÊó†ËÆ∫ÊîªÂáªËÄÖÊòØÈªëÁõíËøòÊòØÁôΩÁõíËÆæÁΩÆ„ÄÇÂ∞ΩÁÆ° DP Â∫îÁî®ÂπøÊ≥õ‰∏îÊúâÊïàÔºå‰ΩÜ‰∏Ä‰∏™Áº∫ÁÇπÊòØÂÆÉÂú®Â§çÊùÇÂ≠¶‰π†‰ªªÂä°‰∏≠Èöæ‰ª•Êèê‰æõÂèØÊé•ÂèóÁöÑÈöêÁßÅÊïàÁî®ÊùÉË°°„ÄÇÊ≠§Â§ñÔºåDP ËøòÂèØ‰ª•Áî®‰∫éÁºìËß£ÂÖ∂‰ªñÂΩ¢ÂºèÁöÑÈöêÁßÅÊîªÂáªÔºåÂ¶ÇÂ±ûÊÄßÊé®Êñ≠ÊîªÂáªÂíåÁâπÊÄßÊé®Êñ≠ÊîªÂáªÔºåÂπ∂‰∏éÂØπÊäóÊ†∑Êú¨ÁöÑÊ®°ÂûãÈ≤ÅÊ£íÊÄßÊúâÂÖ≥„ÄÇ</p>\n<h1 id=\"ÂèØËÉΩÁöÑÊñπÂêë‰ª•ÂèäÂ∫îÁî®\"><a class=\"markdownIt-Anchor\" href=\"#ÂèØËÉΩÁöÑÊñπÂêë‰ª•ÂèäÂ∫îÁî®\">#</a> ÂèØËÉΩÁöÑÊñπÂêë‰ª•ÂèäÂ∫îÁî®</h1>\n<h2 id=\"ÈíàÂØπÊîªÂáªÊñπÂêë\"><a class=\"markdownIt-Anchor\" href=\"#ÈíàÂØπÊîªÂáªÊñπÂêë\">#</a> ÈíàÂØπÊîªÂáªÊñπÂêë</h2>\n<ol>\n<li><strong>ÈíàÂØπÊ≠£ÂàôÂåñÊ®°ÂûãÁöÑÊîªÂáª</strong>ÔºöMIA Á≥ªÁªüÈÄöÂ∏∏‰æùËµñ‰∫éÊú∫Âô®Â≠¶‰π†Á≥ªÁªüÁöÑËøáÊãüÂêàÔºåËÄåÈöèÁùÄÊ≠£ÂàôÂåñÊäÄÊúØÁöÑÂèëÂ±ïÔºåËøô‰∏ÄÂÅáËÆæÊî∂Âà∞ÊåëÊàòÔºõÁõÆÂâçÈíàÂØπËøáÊãüÂêàÊ®°ÂûãÁöÑÊîªÂáª‰ªçÂ§Ñ‰∫éÊú™Áü•Áä∂ÊÄÅ„ÄÇ</li>\n<li><strong>ÈíàÂØπËá™ÁõëÁù£Ê®°ÂûãÁöÑÊîªÂáª</strong>ÔºöÁõÆÂâçÔºåËá™ÁõëÁù£Ê®°ÂûãÂºÄÂßãÂπøÊ≥õÁî®‰∫éËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ª•ÂèäËÆ°ÁÆóÊú∫ËßÜËßâÊñπÈù¢ÔºåÂØπÊ≠§Á±ªÊ®°ÂûãÁöÑÊîªÂáª‰ªçÂ§Ñ‰∫éÊú™Áü•Áä∂ÊÄÅ„ÄÇ</li>\n<li><strong>ÈíàÂØπÂØπÊäóÊÄßÊú∫Âô®Â≠¶‰π†ÁöÑÊîªÂáª</strong>ÔºöÂØπÊäóÊú∫Âô®Â≠¶‰π†‰∏éÊàêÂëòÊé®Êñ≠ÊîªÂáªÂÖ∑Êúâ‰∏ÄÂÆöÁöÑÂÖ±ÂêåÊÄßÂíåÂ∑ÆÂºÇÔºåÂ¶Ç‰ΩïÂ∞ÜÂØπÊäóÊú∫Âô®Â≠¶‰π†ÂíåÊàêÂëòÊé®Êñ≠ÊîªÂáªÁªìÂêàËµ∑Êù•ÊòØÂÖ∂‰∏≠ÂèØËÉΩÁöÑÁ†îÁ©∂ÊñπÂêë<br>\n 4.<strong> ÈíàÂØπÂØπÊØîÂ≠¶‰π†ÔºàContrastive learning aimsÔºâÂíåÂÖÉÂ≠¶‰π†ÔºàMeta-learningÔºâÁ≠âÊñ∞ÂûãÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÁöÑÊîªÂáª</strong>ÔºöÊ≠§Á±ªÊ®°Âûã‰∏é‰º†ÁªüÁöÑÊú∫Âô®Â≠¶‰π†ÊúâÂæàÂ§öÂ∑ÆÂºÇÔºåÈíàÂØπÊ≠§Á±ªÂ∞öÊúâËæÉÂ§öÈ¢ÜÂüü‰∫üÂæÖÁ†îÁ©∂<br>\n 5.<strong> ÈíàÂØπËÅîÈÇ¶Â≠¶‰π†ÁöÑÊîªÂáª</strong>ÔºöÁé∞ÊúâÁöÑ MIA ‰∏ªË¶ÅÈÄÇÁî®‰∫éÂêåË¥®ÂåñÁöÑËÅîÈÇ¶Â≠¶‰π†ÔºåÂØπ‰∫éÂºÇÊûÑÂåñÁöÑËÅîÈÇ¶Â≠¶‰π†Á†îÁ©∂‰∏çÂ§ö<br>\n 6.<strong>MIA Áõ∏ÂÖ≥ÁöÑÂ∫îÁî®</strong>ÔºöÂ¶ÇËÅîÈÇ¶Â≠¶‰π†‰∏≠ÁöÑÊù•Ê∫êÊé®Êñ≠ÊîªÂáª‰ª•ÂèäÊõ¥Âä†Ê∑±ÂÖ•ÁöÑÈöêÁßÅ‰øùÊä§Á†îÁ©∂ÔºåÈÄöËøá MIA ÂÆ°ËÆ°Êï∞ÊçÆËÆ∞ÂΩïÂØπ ML Ê®°ÂûãÁöÑËÆ≠ÁªÉË¥°ÁåÆÁ≠âÂ∫îÁî®„ÄÇ</li>\n</ol>\n<h2 id=\"ÈíàÂØπÈò≤Âæ°ÊñπÂêë\"><a class=\"markdownIt-Anchor\" href=\"#ÈíàÂØπÈò≤Âæ°ÊñπÂêë\">#</a> ÈíàÂØπÈò≤Âæ°ÊñπÂêë</h2>\n<ol>\n<li><strong>ÈíàÂØπÈùûÁõëÁù£Â≠¶‰π†Ê®°ÂûãÁöÑÈò≤Âæ°</strong>ÔºöÈùûÁõëÁù£Â≠¶‰π†Ê®°ÂûãÁî±‰∫éÁº∫‰πèÊï∞ÊçÆÊ†áÁ≠æÔºåÂõ†ËÄåÈöæ‰ª•Â§ÑÁêÜËøáÊãüÂêàÔºåÂú®Ëøô‰∏ÄÊñπÈù¢ÁöÑÁ†îÁ©∂ÂèóÂà∞ÈôêÂà∂</li>\n<li><strong>ÈíàÂØπÁîüÊàêÊ®°ÂûãÁöÑÈò≤Âæ°</strong>ÔºöÂèØËÉΩÁöÑÊñπÂêëÂåÖÊã¨ÈááÁî®Áü•ËØÜËí∏È¶è„ÄÅÂ¢ûÂº∫Â≠¶‰π†Á≠âÊñπÊ≥ïËøõË°åÈò≤Âæ°ÔºåÈÄöËøáÁîüÊàêÊ®°ÂûãËæìÂá∫Áî®‰∫éËÆ≠ÁªÉ‰ª•ÈÅøÂÖçÂéüÂßãÊï∞ÊçÆÁöÑÊ≥ÑÈú≤„ÄÇ</li>\n<li><strong>ÈíàÂØπÈöêÁßÅ‰∏éÊïàÁî®ÁöÑÂπ≥Ë°°</strong>ÔºöÁé∞ÊúâÁöÑÂ∑ÆÂàÜÈöêÁßÅ‰øùÊä§ÈÄöÂ∏∏‰ºöÂØπÂàÜÁ±ªÂô®ÁöÑÊ¢ØÂ∫¶Ê∑ªÂä†Â§ßÈáèÂô™Â£∞ÔºåÁî±Ê≠§‰ºöÈôç‰ΩéÂÖ∂È¢ÑÊµãÁ≤æÂ∫¶ÔºåÂ¶Ç‰ΩïËææÊàêÈöêÁßÅÂíåÊïàÁî®ÁöÑÂπ≥Ë°°‰ªçÂæÖÁ†îÁ©∂</li>\n<li><strong>ÈíàÂØπËÅîÈÇ¶Â≠¶‰π†ÁöÑÈöêÁßÅÈò≤Âæ°</strong>Ôºö ÁõÆÂâçËÅîÈÇ¶Â≠¶‰π†Èù¢‰∏¥ÁùÄË∂äÊù•Ë∂äÂ§öÁöÑÈöêÁßÅÊîªÂáªÔºåÈúÄË¶ÅÂºÄÂèëÁõ∏Â∫îÁöÑÈò≤Âæ°ÊäÄÊúØÔºåÂ∑ÆÂàÜÈöêÁßÅÁ≠âÊäÄÊúØÂú®ËÅîÈÇ¶Â≠¶‰π†‰∏äÁöÑÂ∫îÁî®ÊòØÊú™Êù•ÂèØËÉΩÁöÑ‰∏Ä‰∫õÊñπÂêë</li>\n</ol>\n<p>ÔºàÊú™ÂÆåÔºåÈíàÂØπÊú™Êù•ÁöÑÊñπÂêëÁõÆÂâçÊúâ‰∫õÊÉ≥Ê≥ïÔºåÂêéÈù¢‰ºöÂçïÂºÄ‰∏ÄÁ´†ÁÆÄÂçï‰ªãÁªçÔºâ</p>\n",
            "tags": []
        }
    ]
}